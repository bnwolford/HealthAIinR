[
  {
    "objectID": "misc/Takehome.html#take-home-messages",
    "href": "misc/Takehome.html#take-home-messages",
    "title": "Takehome",
    "section": "Take home messages",
    "text": "Take home messages\n\nNow you know how to read in data (.csv), clean it and explore underlying distributions.\nYou can create publication ready visualizations with ggplot2\nYou know basic statistical commands (t.test) and linear/logistic regression\nYou can use tidymodels to create workflows, recipes, and models to analzye data with a variety of machine learning algorithms\nYou know how to access a high throughput computing cluster for use of sensitive health data\nYou learned how to read documentation of functions and search for answers when you get error messages or don’t know how to approach a problem"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Health AI in R Workshop",
    "section": "",
    "text": "Visit the Course Wiki"
  },
  {
    "objectID": "bonus/MIMIC.html",
    "href": "bonus/MIMIC.html",
    "title": "MIMIC",
    "section": "",
    "text": "Use information from Module 1 and Module 2 to read in sensitive data\nThe Medical Information Mart for Intensive Care III (MIMIC-III) dataset is a large, de-identified and publicly-available collection of medical records. This was published in 2016. We are using the MIMIC-III Clinical Database Demo which is open access. It is downloadable from PhysioNet, but I’ve already put it on HUNT Cloud. It is only for 100 patients, so we have a situation with “big-p, little-n” denoted as p &gt;&gt; n which means there are many more predictors than samples in the data.\nThe newest data set, MIMIC-IV, is also available for certain users that sign the data use agreement and document human ethics training. Physionet calls this “Credientialed Access.”\n\n##call libraries\nlibrary(data.table)\nlibrary(tidyverse)\n\ndata_dir&lt;-\"/path/to/MIMIC/\"\n\n#read in PATIENTS.csv\npatdat&lt;-fread(paste0(data_dir,\"PATIENTS.csv\"))\n\nhead(patdat)\n\n#key for all diagnoses\ndiagnoses&lt;-fread(paste0(data_dir,\"D_ICD_DIAGNOSES.csv\"))\n\n#patient diagnoses\ndiagdat&lt;-fread(paste0(data_dir,\"DIAGNOSES_ICD.csv\"))\n\nIn the future, you may be interested in using synthetic data which avoids some of the limitations and restrictions of senstiive data. You may be interested in Synthea. This is a Synthetic Patient Population Simulator. The goal is to output synthetic, realistic (but not real), patient data and associated health records in a variety of formats.",
    "crumbs": [
      "Home",
      "Bonus",
      "MIMIC"
    ]
  },
  {
    "objectID": "bonus/MIMIC.html#additional-data-set-for-analysis",
    "href": "bonus/MIMIC.html#additional-data-set-for-analysis",
    "title": "MIMIC",
    "section": "",
    "text": "Use information from Module 1 and Module 2 to read in sensitive data\nThe Medical Information Mart for Intensive Care III (MIMIC-III) dataset is a large, de-identified and publicly-available collection of medical records. This was published in 2016. We are using the MIMIC-III Clinical Database Demo which is open access. It is downloadable from PhysioNet, but I’ve already put it on HUNT Cloud. It is only for 100 patients, so we have a situation with “big-p, little-n” denoted as p &gt;&gt; n which means there are many more predictors than samples in the data.\nThe newest data set, MIMIC-IV, is also available for certain users that sign the data use agreement and document human ethics training. Physionet calls this “Credientialed Access.”\n\n##call libraries\nlibrary(data.table)\nlibrary(tidyverse)\n\ndata_dir&lt;-\"/path/to/MIMIC/\"\n\n#read in PATIENTS.csv\npatdat&lt;-fread(paste0(data_dir,\"PATIENTS.csv\"))\n\nhead(patdat)\n\n#key for all diagnoses\ndiagnoses&lt;-fread(paste0(data_dir,\"D_ICD_DIAGNOSES.csv\"))\n\n#patient diagnoses\ndiagdat&lt;-fread(paste0(data_dir,\"DIAGNOSES_ICD.csv\"))\n\nIn the future, you may be interested in using synthetic data which avoids some of the limitations and restrictions of senstiive data. You may be interested in Synthea. This is a Synthetic Patient Population Simulator. The goal is to output synthetic, realistic (but not real), patient data and associated health records in a variety of formats.",
    "crumbs": [
      "Home",
      "Bonus",
      "MIMIC"
    ]
  },
  {
    "objectID": "modules/Module3.html",
    "href": "modules/Module3.html",
    "title": "Module3",
    "section": "",
    "text": "Content for this Module is derived from: * https://swcarpentry.github.io/r-novice-gapminder/08-plot-ggplot2.html * https://www.datanovia.com/en/lessons/ggplot-violin-plot/\n\nChatGPT was used in the creation of examples and explanations.\n\n\n\n\nBy the end of this session, learners will be able to: * Understand the grammar of graphics * Create basic visualizations using ggplot2 * Customize aesthetics, labels, and themes * Use facet_wrap() to explore subgroups\n\n\n\nThe “gg” in ggplot2 stands for grammar of graphics. The plots are built in layers and you can additively string together functions with + to add layers and customize your plots. The key part of making our plot is to tell ggplot how we want to visually represent the data. We do this by adding a new layer to the plot using one of the geom functions.\n\n#read in the data we used in Module 2\n\nusername&lt;-\"YOUR USERNAME HERE\"\npath&lt;-paste0(\"/mnt/work/workbench/\",username,\"/scratch/\",username,\"/data/\")\npath2&lt;-paste0(path,\"sample.csv\")\n\ndat&lt;-read_csv(path2)\n\n# Start with the dataset and aesthetic mapping\nggplot(data = dat, aes(x = Age, y = BloodPressure))\n\n#Nothing appears — because no geometric layer is added yet!\n\n# Add a geometric layer: scatter plot\nggplot(data = dat, aes(x = Age, y = BloodPressure)) +\n  geom_point()\n\n#the plot will show up on the bottom right of your RStudio IDE",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module3"
    ]
  },
  {
    "objectID": "modules/Module3.html#module-3-data-visualization-with-ggplot2",
    "href": "modules/Module3.html#module-3-data-visualization-with-ggplot2",
    "title": "Module3",
    "section": "",
    "text": "Content for this Module is derived from: * https://swcarpentry.github.io/r-novice-gapminder/08-plot-ggplot2.html * https://www.datanovia.com/en/lessons/ggplot-violin-plot/\n\nChatGPT was used in the creation of examples and explanations.\n\n\n\n\nBy the end of this session, learners will be able to: * Understand the grammar of graphics * Create basic visualizations using ggplot2 * Customize aesthetics, labels, and themes * Use facet_wrap() to explore subgroups\n\n\n\nThe “gg” in ggplot2 stands for grammar of graphics. The plots are built in layers and you can additively string together functions with + to add layers and customize your plots. The key part of making our plot is to tell ggplot how we want to visually represent the data. We do this by adding a new layer to the plot using one of the geom functions.\n\n#read in the data we used in Module 2\n\nusername&lt;-\"YOUR USERNAME HERE\"\npath&lt;-paste0(\"/mnt/work/workbench/\",username,\"/scratch/\",username,\"/data/\")\npath2&lt;-paste0(path,\"sample.csv\")\n\ndat&lt;-read_csv(path2)\n\n# Start with the dataset and aesthetic mapping\nggplot(data = dat, aes(x = Age, y = BloodPressure))\n\n#Nothing appears — because no geometric layer is added yet!\n\n# Add a geometric layer: scatter plot\nggplot(data = dat, aes(x = Age, y = BloodPressure)) +\n  geom_point()\n\n#the plot will show up on the bottom right of your RStudio IDE",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module3"
    ]
  },
  {
    "objectID": "modules/Module4.html",
    "href": "modules/Module4.html",
    "title": "Module4",
    "section": "",
    "text": "Content for this Module is derived from: * https://www.emilyzabor.com/survival-analysis-in-r.html * https://www.tidymodels.org/learn/models/parsnip-ranger-glmnet/ ChatGPT was used in the creation of examples and explanations.\n\n\n\nBy the end of this session, learners will be able to: * Perform basic statistical tests in R * Test for model assumptions * Perform survival analysis\n\n\n\nR is primarily a statistical programming language. The focus of this module is not to learn statistics deeply. This content could be a whole semester’s course. The survival analysis alone could be a multi-day workshop. But it’s a quick run through many of the types of statistical tests you can run in R.\n\n#library(tidyverse) if you have installed the whole tidyverse you can load it\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(survival)\nlibrary(survminer)\nlibrary(ggsurvfit)\nlibrary(gtsummary)\n\nWe will use the lung dataset from the survival package as example data. The data contain subjects with advanced lung cancer from the North Central Cancer Treatment Group.\nThe variables are described here\nWhat do the sex, time, status, and ph.ecog variables mean?\n\n# Save the dataset to a variable called \"df\"\ndf&lt;-survival::lung\n\n# Check out the dataset and variables\nglimpse(df)\nsummary(df)\n\n\n\n\nYou can use a t-test when:\n\nthe outcome variable is continuous (e.g., height, blood pressure)\nYou have two groups to compare the means of, such as:\n\n\nTreatment vs control\nMales vs females\nBefore vs after intervention\n\n\nObservations are independent (except for paired t-test)\nThe data are (approximately) normally distributed.\nYou assume equal or unequal variances depending on the type of t-test.\n\nHere we are performing a two-sample t-test comparing the means between 2 groups.\n\n# The variable sex is numeric, let's convert to a factor\ndf&lt;-df %&gt;% mutate(sex=as.factor(sex))\n\n# Perform t-test\nt.test(time~sex,data=df)\n\n# Let's look at the data with a boxplot\nggplot(df, aes(x = sex, y = time)) +\n  geom_boxplot() +\n  labs(title = \"Suvival time for Lung-cancer patients\") + theme_bw()\n\n# Let's check the assumptions by looking at the variance in each group\ndf %&gt;% group_by(sex) %&gt;% summarize(var(time))\nvar.test(time ~ sex, data = df)\n\n# Let's check normality with the Shapiro-Wilk test of normality\ndf %&gt;% filter(sex==1) %&gt;% select(time) %&gt;% unlist() %&gt;% shapiro.test()\ndf %&gt;% filter(sex==2) %&gt;% select(time) %&gt;% unlist() %&gt;% shapiro.test() \n\n#Note that 2 is female and 1 is male\n\n# Both are significant, which means the distributions are unlikely to be normal.\n# It's probably best to try a non-parametric test like the Mann Whitney U (Wilcoxon Rank Sum Test)\n\nwilcox.test(time ~ sex, data=df)\n\n\n\n\nWe will perform a Chi-square test of indepdence to test of the association between two categorical variables. You can think of this as a 2 x 2 table or Y x X table with counts in each cell.\nECOG Performance Status Scale describes a patient’s level of functioning in terms of their ability to care for themself, daily activity, and physical\nAssumptions: 1. Both variables are categorical 2. Observations are independent 3. Expected cell counts should generally be ≥ 5 (for valid p-values) 4. Data are counts (not proportions or percentages)\nA significant p-value (typically &lt; 0.05) suggests there is an association between the variables (they’re not independent).\nIf the assumption’s are violated you can use the Fisher’s Exact Test.\n\n# in base R we can use the function table()\ntable(df$status,df$ph.ecog)\n\nsummary_table &lt;- df %&gt;%\n  group_by(status,ph.ecog) %&gt;%\n  summarize(Count = n())\n\nsummary_table\n\n# Here we see there was a sample with NA for ECOG status. We didn't see this with table() because we have to force it to include NA\ntable(df$status,df$ph.ecog,useNA=\"always\")\n\n# Let's drop the sample with the NA value\nnrow(df)\ndf &lt;- df %&gt;% drop_na(ph.ecog)\nnrow(df)\n\nchisq.test(df$status,df$ph.ecog)\n\n# Because the assumptions are not met (some cells are 0) we should try a Fisher's exact test\nfisher.test(df$status,df$ph.ecog)\n\n\n\n\nUse Case: 1) The dependent variable is continuous (e.g., blood pressure, cholesterol) 2) The independent variable is categorical with three or more groups (e.g., diet group A/B/C, education levels). 3) You want to test whether group means are equal.\nAssumptions: 1) Independence of observations 2) Normality of the dependent variable within each group 3) Equal variances across groups (homogeneity of variance)\nANOVA tells you there is a difference, but not where the difference is — that’s what post-hoc tests are for (e.g., Tukey’s HSD).\n\n# The variable ph.ecog is numeric, let's convert to a factor\ndf&lt;-df %&gt;% mutate(ph.ecog=as.factor(ph.ecog))\n\n# ANOVA\nanova_model &lt;- aov(time ~ ph.ecog, data = df)\nsummary(anova_model)\n\n# Post-hoc test\nTukeyHSD(anova_model)\n\n# Plot\nggplot(df, aes(x = ph.ecog, y = time)) +\n  geom_boxplot() +\n  labs(title = \"ECOG status and survival time in Lung Cancer patients\")\n\n\n\n\nLinear regression is modeling a continuous outcome using one or more predictors.\n\n# First let's look for the association between ECOG and survival time with lm()\nlm_model &lt;- lm(time ~ ph.ecog, data = df)\nsummary(lm_model)\n\n# Now let's add in sex as another predcitor/covariate\nlm_model2 &lt;- lm(time ~ ph.ecog + sex, data = df)\nsummary(lm_model2)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(lm_model)\n\n# plots\nggplot(df, aes(x = ph.ecog, y = time, group=sex)) +\n      geom_point() +\n      geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nIf the outcome is binary, we use a generalized version of linear regression, called logistic regression with glm() function\n\n#logistic regression with glm()\nlogreg&lt;-glm(sex~meal.cal,data=df,family=\"binomial\")\nsummary(logreg)\n\n#diagnostic plots\npar(mfrow = c(2, 2))\nplot(logreg)\n\n\n\n\nThis dataset is actually more appropriately analyzed with survival methods because we have survival time in days and death/censored status.Ignoring censoring like we have done thus far will lead to incorrect estimates of probability of survival, ultimately oversesitmating the overall survival probability.\nsurvival and survminer are popular packages for this type of analysis. ggsurvfit allows for nice data visualization and includes some of the same functionality.\nThe Surv() function from the survival package creates a survival object for use as the response in a model formula. There is one entry for each subject that is the survival time, which is followed by a + if the subject was censored.\n\n# Note that the status is coded in a non-standard way in this dataset. Typically you will see 1=event, 0=censored. Let’s recode it to avoid confusion:\n\ndf &lt;- \n  df %&gt;% \n  mutate(\n    status = recode(status, `1` = 0, `2` = 1)\n  )\n\n# Create survival object\nsurv_obj &lt;- Surv(time = df$time, event = df$status)\n\n#check the first 10 observations\nsurv_obj[1:10]\n\n\n\n\nLet’s generate the overall Kaplan Meier curve for the entire cohort.\nThe survfit() function creates survival curves using the Kaplan-Meier method based on a formula.\nThe key components of this survfit object include:\ntime: the timepoints at which the curve has a step, i.e. at least one event occurred surv: the estimate of survival at the corresponding time\n\n# first let's use survfit() to make a survfit object\n\ns1 &lt;- survfit(Surv(time, status) ~ 1, data = df)\nstr(s1)\n\n# survminer has a plotting function ggsurvplot() which uses ggplot\nggsurvplot(s1, data=df, title = \"Kaplan-Meier Survival Curve\")\n\n# We can also use the ggsurvfit package for plotting\n\n#Functions from ggsurvfit work best if we use built in functions to make the survfit object; note how we can pipe this object into ggsurvfit()\n\nsurvfit2(Surv(time, status) ~ 1, data = df) %&gt;% \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n  )  + \n  add_confidence_interval() +\n   add_risktable()\n\n# we also added a confidence interval and a risk table using grammar of graphics layering\n\n# the gtsummary package can make a publication-ready table estimating probability of 1-year survival\nsurvfit(Surv(time, status) ~ 1, data = df) %&gt;% \n  tbl_survfit(\n    times = 365.25,\n    label_header = \"**1-year survival (95% CI)**\"\n  )",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module4"
    ]
  },
  {
    "objectID": "modules/Module4.html#module-4-statistical-analysis-in-r",
    "href": "modules/Module4.html#module-4-statistical-analysis-in-r",
    "title": "Module4",
    "section": "",
    "text": "Content for this Module is derived from: * https://www.emilyzabor.com/survival-analysis-in-r.html * https://www.tidymodels.org/learn/models/parsnip-ranger-glmnet/ ChatGPT was used in the creation of examples and explanations.\n\n\n\nBy the end of this session, learners will be able to: * Perform basic statistical tests in R * Test for model assumptions * Perform survival analysis\n\n\n\nR is primarily a statistical programming language. The focus of this module is not to learn statistics deeply. This content could be a whole semester’s course. The survival analysis alone could be a multi-day workshop. But it’s a quick run through many of the types of statistical tests you can run in R.\n\n#library(tidyverse) if you have installed the whole tidyverse you can load it\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(survival)\nlibrary(survminer)\nlibrary(ggsurvfit)\nlibrary(gtsummary)\n\nWe will use the lung dataset from the survival package as example data. The data contain subjects with advanced lung cancer from the North Central Cancer Treatment Group.\nThe variables are described here\nWhat do the sex, time, status, and ph.ecog variables mean?\n\n# Save the dataset to a variable called \"df\"\ndf&lt;-survival::lung\n\n# Check out the dataset and variables\nglimpse(df)\nsummary(df)\n\n\n\n\nYou can use a t-test when:\n\nthe outcome variable is continuous (e.g., height, blood pressure)\nYou have two groups to compare the means of, such as:\n\n\nTreatment vs control\nMales vs females\nBefore vs after intervention\n\n\nObservations are independent (except for paired t-test)\nThe data are (approximately) normally distributed.\nYou assume equal or unequal variances depending on the type of t-test.\n\nHere we are performing a two-sample t-test comparing the means between 2 groups.\n\n# The variable sex is numeric, let's convert to a factor\ndf&lt;-df %&gt;% mutate(sex=as.factor(sex))\n\n# Perform t-test\nt.test(time~sex,data=df)\n\n# Let's look at the data with a boxplot\nggplot(df, aes(x = sex, y = time)) +\n  geom_boxplot() +\n  labs(title = \"Suvival time for Lung-cancer patients\") + theme_bw()\n\n# Let's check the assumptions by looking at the variance in each group\ndf %&gt;% group_by(sex) %&gt;% summarize(var(time))\nvar.test(time ~ sex, data = df)\n\n# Let's check normality with the Shapiro-Wilk test of normality\ndf %&gt;% filter(sex==1) %&gt;% select(time) %&gt;% unlist() %&gt;% shapiro.test()\ndf %&gt;% filter(sex==2) %&gt;% select(time) %&gt;% unlist() %&gt;% shapiro.test() \n\n#Note that 2 is female and 1 is male\n\n# Both are significant, which means the distributions are unlikely to be normal.\n# It's probably best to try a non-parametric test like the Mann Whitney U (Wilcoxon Rank Sum Test)\n\nwilcox.test(time ~ sex, data=df)\n\n\n\n\nWe will perform a Chi-square test of indepdence to test of the association between two categorical variables. You can think of this as a 2 x 2 table or Y x X table with counts in each cell.\nECOG Performance Status Scale describes a patient’s level of functioning in terms of their ability to care for themself, daily activity, and physical\nAssumptions: 1. Both variables are categorical 2. Observations are independent 3. Expected cell counts should generally be ≥ 5 (for valid p-values) 4. Data are counts (not proportions or percentages)\nA significant p-value (typically &lt; 0.05) suggests there is an association between the variables (they’re not independent).\nIf the assumption’s are violated you can use the Fisher’s Exact Test.\n\n# in base R we can use the function table()\ntable(df$status,df$ph.ecog)\n\nsummary_table &lt;- df %&gt;%\n  group_by(status,ph.ecog) %&gt;%\n  summarize(Count = n())\n\nsummary_table\n\n# Here we see there was a sample with NA for ECOG status. We didn't see this with table() because we have to force it to include NA\ntable(df$status,df$ph.ecog,useNA=\"always\")\n\n# Let's drop the sample with the NA value\nnrow(df)\ndf &lt;- df %&gt;% drop_na(ph.ecog)\nnrow(df)\n\nchisq.test(df$status,df$ph.ecog)\n\n# Because the assumptions are not met (some cells are 0) we should try a Fisher's exact test\nfisher.test(df$status,df$ph.ecog)\n\n\n\n\nUse Case: 1) The dependent variable is continuous (e.g., blood pressure, cholesterol) 2) The independent variable is categorical with three or more groups (e.g., diet group A/B/C, education levels). 3) You want to test whether group means are equal.\nAssumptions: 1) Independence of observations 2) Normality of the dependent variable within each group 3) Equal variances across groups (homogeneity of variance)\nANOVA tells you there is a difference, but not where the difference is — that’s what post-hoc tests are for (e.g., Tukey’s HSD).\n\n# The variable ph.ecog is numeric, let's convert to a factor\ndf&lt;-df %&gt;% mutate(ph.ecog=as.factor(ph.ecog))\n\n# ANOVA\nanova_model &lt;- aov(time ~ ph.ecog, data = df)\nsummary(anova_model)\n\n# Post-hoc test\nTukeyHSD(anova_model)\n\n# Plot\nggplot(df, aes(x = ph.ecog, y = time)) +\n  geom_boxplot() +\n  labs(title = \"ECOG status and survival time in Lung Cancer patients\")\n\n\n\n\nLinear regression is modeling a continuous outcome using one or more predictors.\n\n# First let's look for the association between ECOG and survival time with lm()\nlm_model &lt;- lm(time ~ ph.ecog, data = df)\nsummary(lm_model)\n\n# Now let's add in sex as another predcitor/covariate\nlm_model2 &lt;- lm(time ~ ph.ecog + sex, data = df)\nsummary(lm_model2)\n\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(lm_model)\n\n# plots\nggplot(df, aes(x = ph.ecog, y = time, group=sex)) +\n      geom_point() +\n      geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nIf the outcome is binary, we use a generalized version of linear regression, called logistic regression with glm() function\n\n#logistic regression with glm()\nlogreg&lt;-glm(sex~meal.cal,data=df,family=\"binomial\")\nsummary(logreg)\n\n#diagnostic plots\npar(mfrow = c(2, 2))\nplot(logreg)\n\n\n\n\nThis dataset is actually more appropriately analyzed with survival methods because we have survival time in days and death/censored status.Ignoring censoring like we have done thus far will lead to incorrect estimates of probability of survival, ultimately oversesitmating the overall survival probability.\nsurvival and survminer are popular packages for this type of analysis. ggsurvfit allows for nice data visualization and includes some of the same functionality.\nThe Surv() function from the survival package creates a survival object for use as the response in a model formula. There is one entry for each subject that is the survival time, which is followed by a + if the subject was censored.\n\n# Note that the status is coded in a non-standard way in this dataset. Typically you will see 1=event, 0=censored. Let’s recode it to avoid confusion:\n\ndf &lt;- \n  df %&gt;% \n  mutate(\n    status = recode(status, `1` = 0, `2` = 1)\n  )\n\n# Create survival object\nsurv_obj &lt;- Surv(time = df$time, event = df$status)\n\n#check the first 10 observations\nsurv_obj[1:10]\n\n\n\n\nLet’s generate the overall Kaplan Meier curve for the entire cohort.\nThe survfit() function creates survival curves using the Kaplan-Meier method based on a formula.\nThe key components of this survfit object include:\ntime: the timepoints at which the curve has a step, i.e. at least one event occurred surv: the estimate of survival at the corresponding time\n\n# first let's use survfit() to make a survfit object\n\ns1 &lt;- survfit(Surv(time, status) ~ 1, data = df)\nstr(s1)\n\n# survminer has a plotting function ggsurvplot() which uses ggplot\nggsurvplot(s1, data=df, title = \"Kaplan-Meier Survival Curve\")\n\n# We can also use the ggsurvfit package for plotting\n\n#Functions from ggsurvfit work best if we use built in functions to make the survfit object; note how we can pipe this object into ggsurvfit()\n\nsurvfit2(Surv(time, status) ~ 1, data = df) %&gt;% \n  ggsurvfit() +\n  labs(\n    x = \"Days\",\n    y = \"Overall survival probability\"\n  )  + \n  add_confidence_interval() +\n   add_risktable()\n\n# we also added a confidence interval and a risk table using grammar of graphics layering\n\n# the gtsummary package can make a publication-ready table estimating probability of 1-year survival\nsurvfit(Surv(time, status) ~ 1, data = df) %&gt;% \n  tbl_survfit(\n    times = 365.25,\n    label_header = \"**1-year survival (95% CI)**\"\n  )",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module4"
    ]
  },
  {
    "objectID": "modules/Module4_working.html",
    "href": "modules/Module4_working.html",
    "title": "Module 4 Blanks",
    "section": "",
    "text": "Content for this Module is derived from: * https://www.emilyzabor.com/survival-analysis-in-r.html * https://www.tidymodels.org/learn/models/parsnip-ranger-glmnet/ ChatGPT was used in the creation of examples and explanations.\n\n\n\nBy the end of this session, learners will be able to: * Perform basic statistical tests in R * Test for model assumptions * Perform survival analysis\n\n\n\nR is primarily a statistical programming language. The focus of this module is not to learn statistics deeply. This content could be a whole semester’s course. The survival analysis alone could be a multi-day workshop. But it’s a quick run through many of the types of statistical tests you can run in R.\n\n#library(tidyverse) if you have installed the whole tidyverse you can load it\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(survival)\nlibrary(survminer)\nlibrary(ggsurvfit)\nlibrary(gtsummary)\n\nWe will use the lung dataset from the survival package as example data. The data contain subjects with advanced lung cancer from the North Central Cancer Treatment Group.\nThe variables are described here\nWhat do the sex, time, status, and ph.ecog variables mean?\n\n# Save the dataset to a variable called \"df\"\ndf&lt;-survival::lung\n\n# Check out the dataset and variables\n\n\n\n\nYou can use a t-test when:\n\nthe outcome variable is continuous (e.g., height, blood pressure)\nYou have two groups to compare the means of, such as:\n\n\nTreatment vs control\nMales vs females\nBefore vs after intervention\n\n\nObservations are independent (except for paired t-test)\nThe data are (approximately) normally distributed.\nYou assume equal or unequal variances depending on the type of t-test.\n\nHere we are performing a two-sample t-test comparing the means between 2 groups.\n\n# The variable sex is numeric, let's convert to a factor\n\n# Perform t-test\n\n# Let's look at the data with a boxplot\n\n\n# Let's check the assumptions by looking at the variance in each group\n\n\n# Let's check normality with the Shapiro-Wilk test of normality\n\n#Note that 2 is female and 1 is male\n\n# Both are significant, which means the distributions are unlikely to be normal.\n# It's probably best to try a non-parametric test like the Mann Whitney U (Wilcoxon Rank Sum Test)\n\n\n\n\nWe will perform a Chi-square test of indepdence to test of the association between two categorical variables. You can think of this as a 2 x 2 table or Y x X table with counts in each cell.\nECOG Performance Status Scale describes a patient’s level of functioning in terms of their ability to care for themself, daily activity, and physical\nAssumptions: 1. Both variables are categorical 2. Observations are independent 3. Expected cell counts should generally be ≥ 5 (for valid p-values) 4. Data are counts (not proportions or percentages)\nA significant p-value (typically &lt; 0.05) suggests there is an association between the variables (they’re not independent).\nIf the assumption’s are violated you can use the Fisher’s Exact Test.\n\n# in base R we can use the function table()\n\n# Here we see there was a sample with NA for ECOG status. We didn't see this with table() because we have to force it to include NA\n\n\n# Let's drop the sample with the NA value\n\n\n\n# Because the assumptions are not met (some cells are 0) we should try a Fisher's exact test\n\n\n\n\nUse Case: 1) The dependent variable is continuous (e.g., blood pressure, cholesterol) 2) The independent variable is categorical with three or more groups (e.g., diet group A/B/C, education levels). 3) You want to test whether group means are equal.\nAssumptions: 1) Independence of observations 2) Normality of the dependent variable within each group 3) Equal variances across groups (homogeneity of variance)\nANOVA tells you there is a difference, but not where the difference is — that’s what post-hoc tests are for (e.g., Tukey’s HSD).\n\n# The variable ph.ecog is numeric, let's convert to a factor\n\n\n# ANOVA\n\n\n# Post-hoc test\n\n\n# Plot\n\n\n\n\nLinear regression is modeling a continuous outcome using one or more predictors.\n\n# First let's look for the association between ECOG and survival time with lm()\n\n\n\n\n# Diagnostic plots\n\n# plots\n\n\n\n\nIf the outcome is binary, we use a generalized version of linear regression, called logistic regression with glm() function\n\n#logistic regression with glm()\n\n#diagnostic plots\n\n\n\n\nThis dataset is actually more appropriately analyzed with survival methods because we have survival time in days and death/censored status.Ignoring censoring like we have done thus far will lead to incorrect estimates of probability of survival, ultimately oversesitmating the overall survival probability.\nsurvival and survminer are popular packages for this type of analysis. ggsurvfit allows for nice data visualization and includes some of the same functionality.\nThe Surv() function from the survival package creates a survival object for use as the response in a model formula. There is one entry for each subject that is the survival time, which is followed by a + if the subject was censored.\n\n# Note that the status is coded in a non-standard way in this dataset. Typically you will see 1=event, 0=censored. Let’s recode it to avoid confusion:\n\n# Create survival object\n\n\n#check the first 10 observations\n\n\n\n\nLet’s generate the overall Kaplan Meier curve for the entire cohort.\nThe survfit() function creates survival curves using the Kaplan-Meier method based on a formula.\nThe key components of this survfit object include:\ntime: the timepoints at which the curve has a step, i.e. at least one event occurred surv: the estimate of survival at the corresponding time\n\n# first let's use survfit() to make a survfit object\n\n\n# survminer has a plotting function ggsurvplot() which uses ggplot\n\n\n# We can also use the ggsurvfit package for plotting\n\n#Functions from ggsurvfit work best if we use built in functions to make the survfit object; note how we can pipe this object into ggsurvfit()\n\n\n\n# we also added a confidence interval and a risk table using grammar of graphics layering\n\n# the gtsummary package can make a publication-ready table estimating probability of 1-year survival",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module 4 Blanks"
    ]
  },
  {
    "objectID": "modules/Module4_working.html#module-4-statistical-analysis-in-r",
    "href": "modules/Module4_working.html#module-4-statistical-analysis-in-r",
    "title": "Module 4 Blanks",
    "section": "",
    "text": "Content for this Module is derived from: * https://www.emilyzabor.com/survival-analysis-in-r.html * https://www.tidymodels.org/learn/models/parsnip-ranger-glmnet/ ChatGPT was used in the creation of examples and explanations.\n\n\n\nBy the end of this session, learners will be able to: * Perform basic statistical tests in R * Test for model assumptions * Perform survival analysis\n\n\n\nR is primarily a statistical programming language. The focus of this module is not to learn statistics deeply. This content could be a whole semester’s course. The survival analysis alone could be a multi-day workshop. But it’s a quick run through many of the types of statistical tests you can run in R.\n\n#library(tidyverse) if you have installed the whole tidyverse you can load it\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(survival)\nlibrary(survminer)\nlibrary(ggsurvfit)\nlibrary(gtsummary)\n\nWe will use the lung dataset from the survival package as example data. The data contain subjects with advanced lung cancer from the North Central Cancer Treatment Group.\nThe variables are described here\nWhat do the sex, time, status, and ph.ecog variables mean?\n\n# Save the dataset to a variable called \"df\"\ndf&lt;-survival::lung\n\n# Check out the dataset and variables\n\n\n\n\nYou can use a t-test when:\n\nthe outcome variable is continuous (e.g., height, blood pressure)\nYou have two groups to compare the means of, such as:\n\n\nTreatment vs control\nMales vs females\nBefore vs after intervention\n\n\nObservations are independent (except for paired t-test)\nThe data are (approximately) normally distributed.\nYou assume equal or unequal variances depending on the type of t-test.\n\nHere we are performing a two-sample t-test comparing the means between 2 groups.\n\n# The variable sex is numeric, let's convert to a factor\n\n# Perform t-test\n\n# Let's look at the data with a boxplot\n\n\n# Let's check the assumptions by looking at the variance in each group\n\n\n# Let's check normality with the Shapiro-Wilk test of normality\n\n#Note that 2 is female and 1 is male\n\n# Both are significant, which means the distributions are unlikely to be normal.\n# It's probably best to try a non-parametric test like the Mann Whitney U (Wilcoxon Rank Sum Test)\n\n\n\n\nWe will perform a Chi-square test of indepdence to test of the association between two categorical variables. You can think of this as a 2 x 2 table or Y x X table with counts in each cell.\nECOG Performance Status Scale describes a patient’s level of functioning in terms of their ability to care for themself, daily activity, and physical\nAssumptions: 1. Both variables are categorical 2. Observations are independent 3. Expected cell counts should generally be ≥ 5 (for valid p-values) 4. Data are counts (not proportions or percentages)\nA significant p-value (typically &lt; 0.05) suggests there is an association between the variables (they’re not independent).\nIf the assumption’s are violated you can use the Fisher’s Exact Test.\n\n# in base R we can use the function table()\n\n# Here we see there was a sample with NA for ECOG status. We didn't see this with table() because we have to force it to include NA\n\n\n# Let's drop the sample with the NA value\n\n\n\n# Because the assumptions are not met (some cells are 0) we should try a Fisher's exact test\n\n\n\n\nUse Case: 1) The dependent variable is continuous (e.g., blood pressure, cholesterol) 2) The independent variable is categorical with three or more groups (e.g., diet group A/B/C, education levels). 3) You want to test whether group means are equal.\nAssumptions: 1) Independence of observations 2) Normality of the dependent variable within each group 3) Equal variances across groups (homogeneity of variance)\nANOVA tells you there is a difference, but not where the difference is — that’s what post-hoc tests are for (e.g., Tukey’s HSD).\n\n# The variable ph.ecog is numeric, let's convert to a factor\n\n\n# ANOVA\n\n\n# Post-hoc test\n\n\n# Plot\n\n\n\n\nLinear regression is modeling a continuous outcome using one or more predictors.\n\n# First let's look for the association between ECOG and survival time with lm()\n\n\n\n\n# Diagnostic plots\n\n# plots\n\n\n\n\nIf the outcome is binary, we use a generalized version of linear regression, called logistic regression with glm() function\n\n#logistic regression with glm()\n\n#diagnostic plots\n\n\n\n\nThis dataset is actually more appropriately analyzed with survival methods because we have survival time in days and death/censored status.Ignoring censoring like we have done thus far will lead to incorrect estimates of probability of survival, ultimately oversesitmating the overall survival probability.\nsurvival and survminer are popular packages for this type of analysis. ggsurvfit allows for nice data visualization and includes some of the same functionality.\nThe Surv() function from the survival package creates a survival object for use as the response in a model formula. There is one entry for each subject that is the survival time, which is followed by a + if the subject was censored.\n\n# Note that the status is coded in a non-standard way in this dataset. Typically you will see 1=event, 0=censored. Let’s recode it to avoid confusion:\n\n# Create survival object\n\n\n#check the first 10 observations\n\n\n\n\nLet’s generate the overall Kaplan Meier curve for the entire cohort.\nThe survfit() function creates survival curves using the Kaplan-Meier method based on a formula.\nThe key components of this survfit object include:\ntime: the timepoints at which the curve has a step, i.e. at least one event occurred surv: the estimate of survival at the corresponding time\n\n# first let's use survfit() to make a survfit object\n\n\n# survminer has a plotting function ggsurvplot() which uses ggplot\n\n\n# We can also use the ggsurvfit package for plotting\n\n#Functions from ggsurvfit work best if we use built in functions to make the survfit object; note how we can pipe this object into ggsurvfit()\n\n\n\n# we also added a confidence interval and a risk table using grammar of graphics layering\n\n# the gtsummary package can make a publication-ready table estimating probability of 1-year survival",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module 4 Blanks"
    ]
  },
  {
    "objectID": "modules/Module1_working.html",
    "href": "modules/Module1_working.html",
    "title": "Module1_blank",
    "section": "",
    "text": "Content for this Module is derived from * https://bookdown.org/rdpeng/rprogdatascience/r-nuts-and-bolts.html * https://hbctraining.github.io/Intro-to-R-flipped/lessons/04_introR_packages.html * https://swcarpentry.github.io/r-novice-inflammation/01-starting-with-data.html#manipulating-data\nChatGPT was used in the creation of examples and explanations.\n\n\n\nBy the end of this session, learners will be able to: * Basics of R programming * Understanding the RStudio IDE * RMarkdown for reproducible research\n\n\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\nA script ends in .R) and a notebook ends in .Rmd\n\n\n\nR has five basic or “atomic” classes of objects: - character - numeric (real numbers) - integer - complex - logical (True/False)\nOnce a variable is created, we can use the variable name to refer to the value it was assigned. The variable name now acts as a tag. To see the value of a variable, we can print it by: 1) typing the name of the variable and hitting return in the Console 2) if your cursor is on a line in the editor window, you can use control-enter (PC) or command-return (Mac) 3) press the Run button above 4) press the green play button in the RMarkdown code chunk In general, R will print to the console any object returned by a function or operation unless we assign it to a variable.\n\n#we use an arrow to assign a value to a variable\n\n\n#we can coerce an object from one class to another\n\nIt is important that we name variables in a way that other users and our future selves will understand. Today, most R programmers 1) start variable names with lower case letters, 2) separate words in variable names with underscores, and 3) use only lowercase letters, underscores, and numbers in variable names. The Tidyverse Style Guide includes a section on this.\nIt’s also important to use the # hash symbol to create comments for other users and our future selves.\nThe most basic type of R object is a vector. Vectors can only contain objects of the same class. A list is represented as a vector but can contain objects of different classes.\n\n#initialize a vector\n\n\n#define a vector with c() function\n\n\n#check what class the variables are\n\n\n#create a data frame\n\n\n\n\n\n#as a calculator\n\n\n\n# View summary statistics\n\n\n# plot data using base R\n\n\n# perform statistical tests\n\n\n\n\nPackages are collections of R functions, data, and compiled code in a well-defined format, created to add specific functionality. You can do a lot with base R, but there are 10,000+ user contributed packages on CRAN. The directories in R where the packages are stored are called the libraries.\nIf you’re using R on your computer, you can use the function install.packages to install a package. You can also use RStudio. However, because of the way workbench is set up in R, we need to use the Terminal to install packages. Return to the JupyterLab tab and select Terminal from the Launcher.\nExecute the following commands: conda activate r-base conda install r-ggplot\n\n#what libraries are loaded in the environment?\n\n\n#now we call the package we installed\n\n\n#now we can use functions from the ggplot2 package\n\nObject-oriented programming is a programming paradigm based on the concept of breaking down a problem or system into objects.\nFunctional programming is a declarative programming paradigm style where one applies pure functions in sequence to solve complex problems. Functions take an input value and produce an output value without being affected by the program.\nR lends itself well to a style of problem solving centered on functions so it can be used as a functional language. However, object orienting programm is also possible in R, using the S3 system for example. These are more advanced computer science concepts which aren’t required to understand, but are good to be aware of.\nHUNT Cloud is a secure environment for analyzing sensitive data.\nUsing sensitive data in biomedical research requires a careful balance between scientific advancement and the protection of individual privacy and rights. Sensitive data often includes personally identifiable information (PII) such as names, dates of birth, or contact details, as well as health-related data like genetic information, diagnoses, test results, or treatment records. Because this type of data can reveal intimate details about a person’s health status and identity, it is subject to strict ethical, legal, and institutional safeguards.\nResearchers must adhere to national and international regulations, such as the General Data Protection Regulation (GDPR) in the European Union. These frameworks define how sensitive data can be collected, stored, shared, and used. For example, under GDPR, data must be processed lawfully, transparently, and only for specified purposes. Researchers are also required to implement data minimization and to ensure that data are accurate, kept up to date, and stored securely.\nIn practice, ethical research with sensitive data involves several layers of protection. First, researchers typically seek approval from an ethics committee, which assesses the risks and benefits of the study and ensures that informed consent is obtained. Consent forms should clearly explain how the data will be used, who will have access, and what measures are in place to protect confidentiality. In some cases, broad consent may be used for biobanks or longitudinal studies like the HUNT Study, but this must still respect participants’ autonomy and rights.\nTo further protect individuals, sensitive data are often de-identified or pseudonymized, meaning that direct identifiers are removed or replaced with codes. However, even de-identified data can carry risks of re-identification, particularly when linked with other datasets. As a result, researchers must employ technical safeguards (such as encryption and secure servers) as well as administrative controls (like restricted access and data-use agreements) to mitigate these risks.\nUltimately, the responsible use of sensitive data is essential for building public trust and ensuring the long-term sustainability of biomedical research. Using sensitive data in biomedical research requires a careful balance between scientific advancement and the protection of individual privacy and rights. Sensitive data often includes personally identifiable information (PII) such as names, dates of birth, or contact details, as well as health-related data like genetic information, diagnoses, test results, or treatment records. Because this type of data can reveal intimate details about a person’s health status and identity, it is subject to strict ethical, legal, and institutional safeguards.\nResearchers must adhere to national and international regulations, such as the General Data Protection Regulation (GDPR) in the European Union or the Health Insurance Portability and Accountability Act (HIPAA) in the United States. These frameworks define how sensitive data can be collected, stored, shared, and used. For example, under GDPR, data must be processed lawfully, transparently, and only for specified purposes. Researchers are also required to implement data minimization and to ensure that data are accurate, kept up to date, and stored securely.\nIn practice, ethical research with sensitive data involves several layers of protection. First, researchers typically seek approval from an ethics committee or institutional review board (IRB), which assesses the risks and benefits of the study and ensures that informed consent is obtained. Consent forms should clearly explain how the data will be used, who will have access, and what measures are in place to protect confidentiality. In some cases, broad consent may be used for biobanks or longitudinal studies, but this must still respect participants’ autonomy and rights.\nTo further protect individuals, sensitive data are often de-identified or pseudonymized, meaning that direct identifiers are removed or replaced with codes. However, even de-identified data can carry risks of re-identification, particularly when linked with other datasets. As a result, researchers must employ technical safeguards (such as encryption and secure servers) as well as administrative controls (like restricted access and data-use agreements) to mitigate these risks.\nUltimately, the responsible use of sensitive data is essential for building public trust and ensuring the long-term sustainability of biomedical research.\nRemember * Never store sensitive data in plaintext without protection. * Always store sensitive data outside version control systems like Git. Use .ignore files to achieve this.",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module1_blank"
    ]
  },
  {
    "objectID": "modules/Module1_working.html#module-1-introduction-to-r",
    "href": "modules/Module1_working.html#module-1-introduction-to-r",
    "title": "Module1_blank",
    "section": "",
    "text": "Content for this Module is derived from * https://bookdown.org/rdpeng/rprogdatascience/r-nuts-and-bolts.html * https://hbctraining.github.io/Intro-to-R-flipped/lessons/04_introR_packages.html * https://swcarpentry.github.io/r-novice-inflammation/01-starting-with-data.html#manipulating-data\nChatGPT was used in the creation of examples and explanations.\n\n\n\nBy the end of this session, learners will be able to: * Basics of R programming * Understanding the RStudio IDE * RMarkdown for reproducible research\n\n\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\nA script ends in .R) and a notebook ends in .Rmd\n\n\n\nR has five basic or “atomic” classes of objects: - character - numeric (real numbers) - integer - complex - logical (True/False)\nOnce a variable is created, we can use the variable name to refer to the value it was assigned. The variable name now acts as a tag. To see the value of a variable, we can print it by: 1) typing the name of the variable and hitting return in the Console 2) if your cursor is on a line in the editor window, you can use control-enter (PC) or command-return (Mac) 3) press the Run button above 4) press the green play button in the RMarkdown code chunk In general, R will print to the console any object returned by a function or operation unless we assign it to a variable.\n\n#we use an arrow to assign a value to a variable\n\n\n#we can coerce an object from one class to another\n\nIt is important that we name variables in a way that other users and our future selves will understand. Today, most R programmers 1) start variable names with lower case letters, 2) separate words in variable names with underscores, and 3) use only lowercase letters, underscores, and numbers in variable names. The Tidyverse Style Guide includes a section on this.\nIt’s also important to use the # hash symbol to create comments for other users and our future selves.\nThe most basic type of R object is a vector. Vectors can only contain objects of the same class. A list is represented as a vector but can contain objects of different classes.\n\n#initialize a vector\n\n\n#define a vector with c() function\n\n\n#check what class the variables are\n\n\n#create a data frame\n\n\n\n\n\n#as a calculator\n\n\n\n# View summary statistics\n\n\n# plot data using base R\n\n\n# perform statistical tests\n\n\n\n\nPackages are collections of R functions, data, and compiled code in a well-defined format, created to add specific functionality. You can do a lot with base R, but there are 10,000+ user contributed packages on CRAN. The directories in R where the packages are stored are called the libraries.\nIf you’re using R on your computer, you can use the function install.packages to install a package. You can also use RStudio. However, because of the way workbench is set up in R, we need to use the Terminal to install packages. Return to the JupyterLab tab and select Terminal from the Launcher.\nExecute the following commands: conda activate r-base conda install r-ggplot\n\n#what libraries are loaded in the environment?\n\n\n#now we call the package we installed\n\n\n#now we can use functions from the ggplot2 package\n\nObject-oriented programming is a programming paradigm based on the concept of breaking down a problem or system into objects.\nFunctional programming is a declarative programming paradigm style where one applies pure functions in sequence to solve complex problems. Functions take an input value and produce an output value without being affected by the program.\nR lends itself well to a style of problem solving centered on functions so it can be used as a functional language. However, object orienting programm is also possible in R, using the S3 system for example. These are more advanced computer science concepts which aren’t required to understand, but are good to be aware of.\nHUNT Cloud is a secure environment for analyzing sensitive data.\nUsing sensitive data in biomedical research requires a careful balance between scientific advancement and the protection of individual privacy and rights. Sensitive data often includes personally identifiable information (PII) such as names, dates of birth, or contact details, as well as health-related data like genetic information, diagnoses, test results, or treatment records. Because this type of data can reveal intimate details about a person’s health status and identity, it is subject to strict ethical, legal, and institutional safeguards.\nResearchers must adhere to national and international regulations, such as the General Data Protection Regulation (GDPR) in the European Union. These frameworks define how sensitive data can be collected, stored, shared, and used. For example, under GDPR, data must be processed lawfully, transparently, and only for specified purposes. Researchers are also required to implement data minimization and to ensure that data are accurate, kept up to date, and stored securely.\nIn practice, ethical research with sensitive data involves several layers of protection. First, researchers typically seek approval from an ethics committee, which assesses the risks and benefits of the study and ensures that informed consent is obtained. Consent forms should clearly explain how the data will be used, who will have access, and what measures are in place to protect confidentiality. In some cases, broad consent may be used for biobanks or longitudinal studies like the HUNT Study, but this must still respect participants’ autonomy and rights.\nTo further protect individuals, sensitive data are often de-identified or pseudonymized, meaning that direct identifiers are removed or replaced with codes. However, even de-identified data can carry risks of re-identification, particularly when linked with other datasets. As a result, researchers must employ technical safeguards (such as encryption and secure servers) as well as administrative controls (like restricted access and data-use agreements) to mitigate these risks.\nUltimately, the responsible use of sensitive data is essential for building public trust and ensuring the long-term sustainability of biomedical research. Using sensitive data in biomedical research requires a careful balance between scientific advancement and the protection of individual privacy and rights. Sensitive data often includes personally identifiable information (PII) such as names, dates of birth, or contact details, as well as health-related data like genetic information, diagnoses, test results, or treatment records. Because this type of data can reveal intimate details about a person’s health status and identity, it is subject to strict ethical, legal, and institutional safeguards.\nResearchers must adhere to national and international regulations, such as the General Data Protection Regulation (GDPR) in the European Union or the Health Insurance Portability and Accountability Act (HIPAA) in the United States. These frameworks define how sensitive data can be collected, stored, shared, and used. For example, under GDPR, data must be processed lawfully, transparently, and only for specified purposes. Researchers are also required to implement data minimization and to ensure that data are accurate, kept up to date, and stored securely.\nIn practice, ethical research with sensitive data involves several layers of protection. First, researchers typically seek approval from an ethics committee or institutional review board (IRB), which assesses the risks and benefits of the study and ensures that informed consent is obtained. Consent forms should clearly explain how the data will be used, who will have access, and what measures are in place to protect confidentiality. In some cases, broad consent may be used for biobanks or longitudinal studies, but this must still respect participants’ autonomy and rights.\nTo further protect individuals, sensitive data are often de-identified or pseudonymized, meaning that direct identifiers are removed or replaced with codes. However, even de-identified data can carry risks of re-identification, particularly when linked with other datasets. As a result, researchers must employ technical safeguards (such as encryption and secure servers) as well as administrative controls (like restricted access and data-use agreements) to mitigate these risks.\nUltimately, the responsible use of sensitive data is essential for building public trust and ensuring the long-term sustainability of biomedical research.\nRemember * Never store sensitive data in plaintext without protection. * Always store sensitive data outside version control systems like Git. Use .ignore files to achieve this.",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module1_blank"
    ]
  },
  {
    "objectID": "modules/Module5.html",
    "href": "modules/Module5.html",
    "title": "Module5",
    "section": "",
    "text": "Sources\nContent for this Module is derived from * https://rebeccabarter.com/blog/2020-03-25_machine_learning * https://education.rstudio.com/blog/2020/02/conf20-intro-ml/ * https://www.r-bloggers.com/2020/05/using-xgboost-with-tidymodels/#google_vignette * https://rpubs.com/leoohyama/tidymodelstitanic\nChatGPT was used in the creation of examples and explanations.\n\n\nObjectives\nBy the end of the session, participants will be able to: * Understand key concepts in AI & machine learning. * Perform data preprocessing for ML tasks. * Train and evaluate basic models using caret, tidymodels, randomForest, and xgboost. * Interpret key model evaluation metrics.\n\n\nIntroduction\nAI versus Machine Learning\nMachine learning is a subset of AI. “Artificial intelligence refers to the general ability of computers to emulate human thought and perform tasks in real-world environments, while machine learning refers to the technologies and algorithms that enable systems to identify patterns, make decisions, and improve themselves through experience and data.” Deep learning is a type of machine learning using neural networks. You can read more:\n\nhttps://ai.engineering.columbia.edu/ai-vs-machine-learning/\nhttps://professionalprograms.mit.edu/blog/technology/machine-learning-vs-artificial-intelligence/\nhttps://www.ibm.com/think/topics/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks\n\n\n\nMachine learning in R\nThe original package for machine learning in R is caret: Caret stands for Classification And Regression Training. More recently, tidymodels was to developed to use the grammar of the tidyverse as a collection of modelling packages. caret is more widely used while tidymodels offers a modern, modular approach.\nJust as tidyverse has libraries within it such as dplyr and tidyr, tidymodels has libraries within it too. The key ones are:\nrsample:sample splitting (e.g. train/test or cross-validation)\nrecipes:pre-processing\nparsnip: specifying the model\nyardstick: evaluating the model\n\nlibrary(tidymodels)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(randomForest)\nlibrary(ranger)\nlibrary(xgboost)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(workflows)\nlibrary(tune)\nlibrary(mlbench)\n\n\n\nExploratory Data Analysis and Pre-processing\nThe first step is an exploratory data analysis—-plots and summaries of the data to understand the variables and their distributions. However, we have done that in previous modules.\nWe will use the Pima Indian Women’s diabetes dataset which contains information on 768 Pima Indian women’s diabetes status, as well as many predictive features. This is built into the mlbench library.\n\n#load data from the mlbench package\ndata(\"PimaIndiansDiabetes\")\ndf &lt;- PimaIndiansDiabetes\n\n#let's look at the data\nglimpse(df)\nstr(df)\nsummary(df)\n\n#histogram of insulin values\nggplot(df) +\n  geom_histogram(aes(x = insulin))\n\n#what do you think about the overabundance of values of 0?\n#it turns out that the NA values are coded as 0 in this dataset\n\n#lets convert 0 to NA for the relevant columns\ndf_clean &lt;- df %&gt;%\n  mutate_at(vars(triceps, glucose, pressure, insulin, mass), \n            function(.var) { \n              if_else(condition = (.var == 0), # if true (i.e. the entry is 0)\n                      true = as.numeric(NA),  # replace the value with NA\n                      false = .var # otherwise leave it as it is\n                      )\n            })\n\n\n\ndata split\nTraining data is used for the model training and hyperparameter tuning. Once trained, the model can be evaluated against test data to assess accuracy.\nThis split can be done automatically using the inital_split() function (from rsample) which creates a special “split” object.\n\nset.seed(1234) #set a seed so we have a reproducible split\n\n# split the data into training (75%) and testing (25%)\ndf_split &lt;- initial_split(df_clean, prop = 3/4)\ndf_split\n\n# extract training and testing sets\ndf_train &lt;- training(df_split)\ndf_test &lt;- testing(df_split)\n\n# create cross validation object from training data\ndf_cv &lt;- vfold_cv(df_train)\n\n#how many folds does this make by default?\n?vfold_cv\n\n\n\nDefine a recipe\nCreating a recipe has two parts which we layer with pipes:\n\nSpecify the formula (recipe()): specify the outcome variable and predictor variables\nSpecify pre-processing steps: define the pre-processing steps, such as imputation of missing data, creating dummy variables, scaling, etc.\n\nNote we use the “role selections” as arguments to the pre-processing steps. They specify that we want to apply the step to “all numeric” variables or “all predictor variables”.\n\n# define the recipe\nmy_recipe &lt;- \n  # which consists of the formula (outcome ~ predictors)\n  recipe(diabetes ~ pregnant + glucose + pressure + triceps + \n           insulin + mass + pedigree + age, \n         data = df_clean) %&gt;%\n  # and some pre-processing steps\n  step_normalize(all_numeric()) %&gt;%\n  step_impute_knn(all_predictors())\n\n?recipes::selections\n\n#look at how many predictor variables we’ve specified and the steps we’ve specified \nsummary(my_recipe)\nmy_recipe\n\n# preprocessing\ndf_train_preprocessed &lt;- my_recipe %&gt;%\n  # apply the recipe to the training data\n  prep(df_train) %&gt;%\n  # extract the pre-processed training dataset\n  juice()\ndf_train_preprocessed\n\nNote that we used the original df_clean data object rather than the df_train object or the df_split object. We could have used any of these. At this point recipes only takes the names and roles of the outcome and predictor variable from the data object. We will apply this recipe to specific datasets later.\n\n\nSpecify model with parsnip\nparsnip offers a unified interface for the massive variety of models that exist in R. This means that you only have to learn one way of specifying a model, and you can use this specification and have it generate a linear model, a random forest model, a support vector machine model, and more with a single line of code.\nFor model specification you need:\n\nThe model type: what kind of model you want to fit, set using a different function depending on the model, such as rand_forest() for random forest, logistic_reg() for logistic regression, svm_poly() for a polynomial SVM model etc. The full list of models available via parsnip can be found here.\nThe arguments: the model parameter values set using set_args().\nThe engine: the underlying package the model should come from (e.g. “ranger” for the fast C++ implementation of Random Forest or randomForest for the original R implementation), set using set_engine().\nThe mode: the type of prediction - since several packages can do both classification (binary/categorical prediction) and regression (continuous prediction), set using set_mode().\n\nA random forest is a machine learning algorithm that uses an ensemble of decision trees to make predictions. It works by training multiple decision trees on different subsets of the data and then combining their outputs to arrive at a final prediction. This ensemble approach generally leads to higher accuracy and better generalization compared to using a single decision tree.\nFor random forest, we can tune hyperparameters like mtry, the number of randomly selected predictors to consider at each split, or trees, the number of trees in the forest.\n\n#random forest from ranger\n\nrf_model &lt;- \n  # specify that the model is a random forest\n  rand_forest() %&gt;%\n  # specify that the `mtry` parameter needs to be tuned\n  set_args(mtry = tune()) %&gt;%\n  # select the engine/package that underlies the model\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  # choose either the continuous regression or binary classification mode\n  set_mode(\"classification\") \n\n#Note: If you want to be able to examine the variable importance of your final model later, you will need to set importance argument when setting the engine. For ranger, the importance options are \"impurity\" or \"permutation\".\n\n# logistic regression from glm model\n\nlr_model &lt;- \n  # specify that the model is logistic regression\n  logistic_reg() %&gt;%\n  # select the engine/package that underlies the model\n  set_engine(\"glm\") %&gt;%\n  # choose either the continuous regression or binary classification mode\n  set_mode(\"classification\") \n\n\n\nWorkflow\nNote, it is only when we tune the parameters or fit the model that the recipe and model frameworks are actually implemented.\n\n# Initiate a workflow, add recipe and model to it\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(my_recipe) %&gt;%\n  add_model(rf_model)\n\n\n\nHyperparameter tuning\nWe will perform tuning on the cross-validation object. We will specify the range of mtry values and then we add a tuning layer to the workflow using tune_grid() from the tune package. We will consider two metrics from the yardstick package—accurary and area under the receiver operator curve.\nWe will explore the results with collect_metrics().\n\n# specify which values to try\nrf_grid &lt;- expand.grid(mtry = c(3, 4, 5))\n# extract results\nrf_tune_results &lt;- rf_workflow %&gt;%\n  tune_grid(resamples = df_cv, #CV object\n            grid = rf_grid, # grid of values to try\n            metrics = metric_set(accuracy, roc_auc)\n            )\n\n# print results\nrf_tune_results %&gt;%\n  collect_metrics()\n\nNow let’s also tune the trees hyperparameter at the same time as mtry.\n\n#first we edit the model\nrf_model &lt;- \n  # specify that the model is a random forest\n  rand_forest() %&gt;%\n  # specify that the `mtry` and `trees` parameter needs to be tuned\n  set_args(mtry = tune(),trees=tune()) %&gt;%\n  # select the engine/package that underlies the model\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  # choose either the continuous regression or binary classification mode\n  set_mode(\"classification\") \n\n#call the workflow again\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(my_recipe) %&gt;%\n  add_model(rf_model)\n\n#now we add trees to the grid\nrf_grid &lt;- expand.grid(mtry = c(3, 4, 5), trees = c(100, 500,1000))\n# extract results\nrf_tune_results2 &lt;- rf_workflow %&gt;%\n  tune_grid(resamples = df_cv, #CV object\n            grid = rf_grid, # grid of values to try\n            metrics = metric_set(accuracy, roc_auc)\n            )\n\n# print results\nrf_tune_results2 %&gt;%\n  collect_metrics()\n\n# Let's find the maximum AUROC and accuracy\nrf_tune_results2 %&gt;%\n  collect_metrics() %&gt;% group_by(.metric) %&gt;% summarize(max(mean))\n\n# which parameters do those correspond to?\n rf_tune_results2 %&gt;%\n  collect_metrics() %&gt;% group_by(.metric) %&gt;% slice_max(mean, n = 1)\n \n # Select best by AUROC\nbest_auroc &lt;- select_best(rf_tune_results2, metric = \"roc_auc\")\nbest_auroc\n\n# Select best by accuracy\nbest_acc &lt;- select_best(rf_tune_results2, metric = \"accuracy\")\nbest_acc\n\nAs you can see, the best AUROC and best accuracy give us different hyperparameters. What to do?\nPick the metric that actually matters for your problem * Accuracy is best if false positives and false negatives cost roughly the same and your classes are balanced. * AUROC is better if you care about ranking cases correctly (e.g., risk scores) or you have class imbalance. It evaluates how well the model separates the classes regardless of a specific threshold. * If you’re in biomedical research, AUROC is often more appropriate than accuracy, because outcomes like disease presence are usually imbalanced, and you care about discrimination rather than just the raw percent correct.\nIdeally we would report both metrics but select based on our primary metric of interest. If accuracy and AUROC pull you toward wildly different parameter sets, it may be worth checking why — sometimes it’s due to overfitting to noise in one metric.\n\n# We want to add a layer to our workflow that corresponds to the tuned parameter, i.e. sets mtry to be the value that yielded the best results. If you didn’t tune any parameters, you can skip this step.\nparam_final &lt;- rf_tune_results2 %&gt;%\n  select_best(metric = \"roc_auc\")\nparam_final\n\n# Finalize workflow with best AUROC params\nrf_workflow &lt;- rf_workflow %&gt;%\n  finalize_workflow(param_final)\n\n\n\nFit the model\nNow we’ve defined the recipe, model, and tuned the model’s parameters. Since all of this information is in the workflow object, we will apply the last_fit() function to our workflow and our train/test split object. This will automatically train the model specified by the workflow using the training data, and produce evaluations based on the test set.\nNote that the fit object is a data-frame-like object–a tibble with list columns.\nYou can also extract the test set predictions themselves using the collect_predictions() function. Note that there are 192 rows in the predictions object, which corresponds to the number of rows in the test set.\n\nrf_fit &lt;- rf_workflow %&gt;%\n  # fit on the training set and evaluate on test set\n  last_fit(df_split)\n\nrf_fit\n\n# Extract performance of final model on the test set\ntest_performance &lt;- rf_fit %&gt;% collect_metrics()\ntest_performance\n\n# Generate predictions from the test set\ntest_predictions &lt;- rf_fit %&gt;% collect_predictions()\ntest_predictions\nnrow(test_predictions)\nnrow(df_test)\n\n# Generate a confusion matrix\ntest_predictions %&gt;% \n  conf_mat(truth = diabetes, estimate = .pred_class)\n\n# Plot distributions of the predicted probability distributions for each class\n\ntest_predictions %&gt;%\n  ggplot() +\n  geom_density(aes(x = .pred_pos, fill = diabetes), \n               alpha = 0.5) +\n  scale_fill_manual(values=c(\"goldenrod3\",\"darkblue\")) + \n  theme_bw()\n\n\n\nPredicting with our final model\nNow that we have our final model, we want to train it on your full dataset and then use it to predict the response for new data.\nThe object from fit contains a few things including the ranger object trained with the parameters established through the workflow based on the joined test/train data (which is in the original clean data frame).\n\nfinal_model &lt;- fit(rf_workflow, df_clean)\n\nfinal_model\n\n# Let's predict risk of disease for a new patient\n\nnew_woman &lt;- tribble(~pregnant, ~glucose, ~pressure, ~triceps, ~insulin, ~mass, ~pedigree, ~age,\n                     2, 95, 70, 31, 102, 28.2, 0.67, 47)\nnew_woman\n\npredict(final_model, new_data = new_woman)\n\n\n\nEvaluating Variable Importance\n\n# Extract the underlying ranger model\nrf_fit_object &lt;- extract_fit_parsnip(final_model)$fit\n\n# Get variable importance\nvar_imp &lt;- ranger::importance(rf_fit_object)\n\n# Print sorted importance\nsort(var_imp, decreasing = TRUE)\n\n#let's plot the variable importance\n\n#get the results in a data frame for plotting\nvar_imp_df &lt;- ranger::importance(rf_fit_object) %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(var = \"variable\") %&gt;%\n  rename(importance = 2) %&gt;%\n  arrange(desc(importance))\n\n# point plot\nggplot(var_imp_df, aes(x = reorder(variable, importance), y = importance)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(\n    title = \"Variable Importance from Random Forest\",\n    x = \"Predictor\",\n    y = \"Importance\"\n  ) +\n  theme_minimal()\n\n\n\nChallenge:\nThe goal is to use what we have learned with tidymodels, workflow, and tune and run xgboost and glm. Note, XGBoost is robust against highly skewed and/or correlated data, so the amount of preprocessing required is minimal. It is another type of supervised machine learning algorithm.\n\n# Model specification with tuneable parameters\nxgb_spec &lt;- boost_tree(\n  trees = tune(),\n  tree_depth = tune(),\n  learn_rate = tune(),\n  loss_reduction = tune(),\n  sample_size = tune(),\n  mtry = tune()\n) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"xgboost\")\n\n# Preprocessing recipe\nxgb_recipe &lt;- recipe(diabetes ~ pregnant + glucose + pressure + triceps + \n           insulin + mass + pedigree + age, \n         data = df_clean) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%   # one-hot encode if needed\n  step_zv(all_predictors())                  # remove zero variance cols\n\n# Workflow\nxgb_wf &lt;- workflow() %&gt;%\n  add_model(xgb_spec) %&gt;%\n  add_recipe(xgb_recipe)\n\n# Grid of parameters\nxgb_grid &lt;- grid_latin_hypercube(\n  trees(range = c(100, 1000)),\n  tree_depth(range = c(1, 10)),\n  learn_rate(range = c(0.001, 0.3)),\n  loss_reduction(range = c(0, 5)),   # gamma\n  sample_size = sample_prop(),       # subsample\n  mtry(range = c(1, ncol(df_train) - 1)),\n  size = 20\n)\n\n# Metrics\nmulti_metrics &lt;- metric_set(accuracy, roc_auc)\n\n# Tune model\nxgb_tune &lt;- tune_grid(\n  xgb_wf,\n  resamples = df_cv,\n  grid = xgb_grid,\n  metrics = multi_metrics\n)\n\n# View results\nxgb_tune %&gt;% collect_metrics()\n\n# Select best by AUROC\nbest_auc &lt;- select_best(xgb_tune, metric = \"roc_auc\")\nbest_auc\n\n# Finalize workflow with best AUROC params\nfinal_xgb &lt;- finalize_workflow(xgb_wf, best_auc)\n\n# Fit on full training set\nfinal_fit &lt;- fit(final_xgb, data = df_train)\n\n# Evaluate on test set\ndf_test %&gt;%\n  predict(final_fit, ., type = \"prob\") %&gt;%\n  bind_cols(df_test) %&gt;%\n  roc_auc(truth = diabetes,.pred_neg)  # AUROC\n\ndf_test %&gt;%\n  predict(final_fit, ., type = \"class\") %&gt;%\n  bind_cols(df_test) %&gt;%\n  accuracy(truth = diabetes, estimate = .pred_class)  # Accuracy\n\ntest_results &lt;- predict(final_fit, df_test, type = \"prob\") %&gt;%\n  bind_cols(df_test %&gt;% select(diabetes))  # event level\n\n# Calculate ROC curve data\nroc_data &lt;- roc_curve(test_results, truth = diabetes, .pred_neg)\n\n# Plot ROC curve\nautoplot(roc_data) +  \n  ggtitle(\"ROC Curve for Diabetes Model\") +\n  theme_minimal()",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module5"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Home",
      "License"
    ]
  },
  {
    "objectID": "LICENSE.html#instructional-material",
    "href": "LICENSE.html#instructional-material",
    "title": "",
    "section": "Instructional Material",
    "text": "Instructional Material\nAll instructional material is made available under the Creative Commons Attribution license. The following is a human-readable summary of (and not a substitute for) the full legal text of the CC BY 4.0 license.\nYou are free:\n\nto Share—copy and redistribute the material in any medium or format\nto Adapt—remix, transform, and build upon the material\n\nfor any purpose, even commercially.\nThe licensor cannot revoke these freedoms as long as you follow the license terms.\nUnder the following terms:\n\nAttribution—You must give appropriate credit, mentioning that your work is derived from work that is Copyright © Health AI in R Course Instructors and, where practical, linking to this repository, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\nNo additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. With the understanding that:\nNotices:\n\nYou do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.",
    "crumbs": [
      "Home",
      "License"
    ]
  },
  {
    "objectID": "LICENSE.html#software",
    "href": "LICENSE.html#software",
    "title": "",
    "section": "Software",
    "text": "Software\nExcept where otherwise noted, the example programs and other software provided by Health AI in R Course Instructors are made available under the OSI-approved MIT license.\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",
    "crumbs": [
      "Home",
      "License"
    ]
  },
  {
    "objectID": "activities/Activity2_filled.html",
    "href": "activities/Activity2_filled.html",
    "title": "Activity2 Filled",
    "section": "",
    "text": "Use information from previous modules to start analyzing a new large dataset. We’ve downloaded Artificial data from the UK National Healthcare System from this website\nYour objective is to perform exploratory data analysis with the artificial data, which is very like real sensitive data, and use some classic statistical methods.\n\n##call libraries\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\n#remember to make this path make sense efor the working directory you are currently in\ndata_dir&lt;-\"artificial_hes_apc_0102.csv\"\n\n#read in .csv\ndf&lt;-read_csv(data_dir)\n\nglimpse(df)\n\n#make a smaller dataset to work with\nsub&lt;-df %&gt;% select(DIAG_3_01, ADMIDATE, DISDATE,PSEUDO_HESID,MYDOB, SEX)\n\n#calculate number of days someone was admitted\nsub&lt;-sub %&gt;% \n  mutate(duration=as.numeric(DISDATE-ADMIDATE))\n\n#pre processing\nsummary(sub$duration)\n\n#remove outliers\nsub2 &lt;- sub %&gt;% filter(duration&lt;500)\n\n#linear model\nlm_obj&lt;-lm(duration~DIAG_3_01,data=sub2)\nsummary(lm_obj)\nplot(lm_obj)\n\ncoef_df&lt;-data.frame(lm_obj$coefficients) \ncoef_df&lt;- coef_df%&gt;%\n  mutate(variable=row.names(coef_df))\n\nggplot(coef_df,aes(x=lm_obj.coefficients,y=variable)) + geom_point()\n\n\n#### use tidymodels\n\n# Split data into training and testing\nset.seed(123)\ndata_split &lt;- initial_split(sub2, prop = 0.8) #split 80/20\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n# Recipe: outcome = duration, predictors = diagnosis_code\nlm_recipe &lt;- recipe(duration ~ DIAG_3_01, data = train_data) %&gt;%\n  step_unknown() %&gt;% #assigns a missing value in a factor level to unknown\n  step_dummy(all_nominal_predictors()) # convert diagnosis_code to dummies\n\n# Model specification: linear regression\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n# Workflow\nlm_workflow &lt;- workflow() %&gt;%\n  add_recipe(lm_recipe) %&gt;%\n  add_model(lm_spec)\n\n# Fit model on training data\nlm_fit &lt;- lm_workflow %&gt;%\n  fit(data = train_data)\n\n# View results\nlm_fit_tidy&lt;-lm_fit %&gt;% tidy()\nlm_fit_tidy %&gt;% slice_max(estimate, n = 5)\n\n# Predict on test set\npredictions &lt;- predict(lm_fit, test_data) %&gt;%\n  bind_cols(test_data)\n\n# Evaluate model performance\nmetrics(predictions, truth = duration, estimate = .pred)",
    "crumbs": [
      "Home",
      "Activities",
      "Activity2 Filled"
    ]
  },
  {
    "objectID": "activities/Activity2_filled.html#activity-2",
    "href": "activities/Activity2_filled.html#activity-2",
    "title": "Activity2 Filled",
    "section": "",
    "text": "Use information from previous modules to start analyzing a new large dataset. We’ve downloaded Artificial data from the UK National Healthcare System from this website\nYour objective is to perform exploratory data analysis with the artificial data, which is very like real sensitive data, and use some classic statistical methods.\n\n##call libraries\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\n#remember to make this path make sense efor the working directory you are currently in\ndata_dir&lt;-\"artificial_hes_apc_0102.csv\"\n\n#read in .csv\ndf&lt;-read_csv(data_dir)\n\nglimpse(df)\n\n#make a smaller dataset to work with\nsub&lt;-df %&gt;% select(DIAG_3_01, ADMIDATE, DISDATE,PSEUDO_HESID,MYDOB, SEX)\n\n#calculate number of days someone was admitted\nsub&lt;-sub %&gt;% \n  mutate(duration=as.numeric(DISDATE-ADMIDATE))\n\n#pre processing\nsummary(sub$duration)\n\n#remove outliers\nsub2 &lt;- sub %&gt;% filter(duration&lt;500)\n\n#linear model\nlm_obj&lt;-lm(duration~DIAG_3_01,data=sub2)\nsummary(lm_obj)\nplot(lm_obj)\n\ncoef_df&lt;-data.frame(lm_obj$coefficients) \ncoef_df&lt;- coef_df%&gt;%\n  mutate(variable=row.names(coef_df))\n\nggplot(coef_df,aes(x=lm_obj.coefficients,y=variable)) + geom_point()\n\n\n#### use tidymodels\n\n# Split data into training and testing\nset.seed(123)\ndata_split &lt;- initial_split(sub2, prop = 0.8) #split 80/20\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n# Recipe: outcome = duration, predictors = diagnosis_code\nlm_recipe &lt;- recipe(duration ~ DIAG_3_01, data = train_data) %&gt;%\n  step_unknown() %&gt;% #assigns a missing value in a factor level to unknown\n  step_dummy(all_nominal_predictors()) # convert diagnosis_code to dummies\n\n# Model specification: linear regression\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\n# Workflow\nlm_workflow &lt;- workflow() %&gt;%\n  add_recipe(lm_recipe) %&gt;%\n  add_model(lm_spec)\n\n# Fit model on training data\nlm_fit &lt;- lm_workflow %&gt;%\n  fit(data = train_data)\n\n# View results\nlm_fit_tidy&lt;-lm_fit %&gt;% tidy()\nlm_fit_tidy %&gt;% slice_max(estimate, n = 5)\n\n# Predict on test set\npredictions &lt;- predict(lm_fit, test_data) %&gt;%\n  bind_cols(test_data)\n\n# Evaluate model performance\nmetrics(predictions, truth = duration, estimate = .pred)",
    "crumbs": [
      "Home",
      "Activities",
      "Activity2 Filled"
    ]
  },
  {
    "objectID": "activities/Activity1_filled.html",
    "href": "activities/Activity1_filled.html",
    "title": "Activity 1",
    "section": "",
    "text": "Pair programming is a technique where two programmers work together at one workstation. One programmer, the driver, writes the code, while the other, the navigator, reviews the code as it’s being written and offers suggestions. The goal is to learn from each other. It’s best practice to switch driver and navigator roles roughly every 10 minutes or whenever you come to a natural break point.\n\n\n\nFor the hands-on application of what you’ve been learning, we are going to use artificial data sets from the United Kingdom’s National Health Service. These are downloaded from here.\nWe need a data dictionary to understand what the columns represent. We are focusing on the admitted patient care data today. Use the excel file HES+TOS+V2.04 and use the HES APC TOS tab.\nWe are using the sample which is 10,000 rows instead of 1 million rows in the “full” dataset. This is due to time and memory constraints on the lab, but you could absolutely do this type of analysis on large sensitive data like the full sample available.\nFirst, we need to go to Terminal and copy the data from a shared folder to your own folder. Remember to replace $USER with your username. cp -r /mnt/work/data/artificial_hes_apc_202302_v1_sample/ /mnt/work/workbench/$USER/\n\n##call libraries\n#library(c(tidyr,dplyr,readr,ggplot2))\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\n\ndata_dir&lt;-\"activities/artificial_hes_apc_0102.csv\"\n\n\n#read in .csv\ndf&lt;-read_csv(data_dir)\n\nglimpse(df)\nhead(df)\n\nLet’s focus on the diagnoses (DIAG_3_01) the admission date (ADMIDATE); the discharge date (DISDATE); Date of Birth (MYDOB); and sex (SEX). What are the first steps we would take if we want to study what diagnoses are associated with a longer hospital admission time?\nThe tibble shows the dates in a “Date” class. To manipualte dates we can use the Sys.Date functionality or we can use the lubridate package but there is also clock which offers a “tidy” way of managing dates and times.\n\n#make a smaller dataset to work with\nsub&lt;-df %&gt;% select(DIAG_3_01, ADMIDATE, DISDATE,PSEUDO_HESID,MYDOB, SEX)\n\n#calculate number of days someone was admitted\nsub&lt;-sub %&gt;% mutate(duration=as.numeric(DISDATE-ADMIDATE))\n\n#how many different diagnoses do we have in the dataset?\nsub %&gt;% summarise(unique_count = n_distinct(DIAG_3_01))\n \n#which diagnosis is most frequent?\nsub %&gt;% group_by(DIAG_3_01) %&gt;% summarize(count=n()) %&gt;% arrange(desc(count))\n\n#create a histogram of the diagnoses\nsub %&gt;% ggplot(aes(DIAG_3_01)) + geom_bar()\nsub %&gt;% group_by(DIAG_3_01) %&gt;% \n  summarize(diag_count=n()) %&gt;% \n  ggplot(aes(diag_count)) + geom_histogram()\n\n#create a histogram of the duration of admission\nsub %&gt;% group_by(duration) %&gt;% summarize(duration_count=n()) %&gt;% \n  ggplot(aes(duration_count)) + geom_histogram(binwidth=2) + xlim(0,50)\n\nsub %&gt;% group_by(duration) %&gt;% summarize(duration_count=n()) \n\n#how can you calculate the age at admission using the month and year DOB? Note the latter is in a character format instead of a date format\nlibrary(lubridate)\nsub2 &lt;- sub %&gt;% mutate(birth_date = as_date(MYDOB, format = \"%m%Y\")) %&gt;% \n  mutate(age_admit = ADMIDATE - birth_date) %&gt;% \n  mutate(age_admit_years = as.numeric(age_admit)/365.25)\n\nsub2  %&gt;% ggplot(aes(age_admit_years)) + geom_histogram()\n\nThis data could be used to ask a lot of questions. What kind of hypotheses do you have based on what you have seen so far? Think about what you might like to research using AI methods. We will continue with this dataset in the next two activities.",
    "crumbs": [
      "Home",
      "Activities",
      "Activity 1"
    ]
  },
  {
    "objectID": "activities/Activity1_filled.html#activity-1",
    "href": "activities/Activity1_filled.html#activity-1",
    "title": "Activity 1",
    "section": "",
    "text": "Pair programming is a technique where two programmers work together at one workstation. One programmer, the driver, writes the code, while the other, the navigator, reviews the code as it’s being written and offers suggestions. The goal is to learn from each other. It’s best practice to switch driver and navigator roles roughly every 10 minutes or whenever you come to a natural break point.\n\n\n\nFor the hands-on application of what you’ve been learning, we are going to use artificial data sets from the United Kingdom’s National Health Service. These are downloaded from here.\nWe need a data dictionary to understand what the columns represent. We are focusing on the admitted patient care data today. Use the excel file HES+TOS+V2.04 and use the HES APC TOS tab.\nWe are using the sample which is 10,000 rows instead of 1 million rows in the “full” dataset. This is due to time and memory constraints on the lab, but you could absolutely do this type of analysis on large sensitive data like the full sample available.\nFirst, we need to go to Terminal and copy the data from a shared folder to your own folder. Remember to replace $USER with your username. cp -r /mnt/work/data/artificial_hes_apc_202302_v1_sample/ /mnt/work/workbench/$USER/\n\n##call libraries\n#library(c(tidyr,dplyr,readr,ggplot2))\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\n\ndata_dir&lt;-\"activities/artificial_hes_apc_0102.csv\"\n\n\n#read in .csv\ndf&lt;-read_csv(data_dir)\n\nglimpse(df)\nhead(df)\n\nLet’s focus on the diagnoses (DIAG_3_01) the admission date (ADMIDATE); the discharge date (DISDATE); Date of Birth (MYDOB); and sex (SEX). What are the first steps we would take if we want to study what diagnoses are associated with a longer hospital admission time?\nThe tibble shows the dates in a “Date” class. To manipualte dates we can use the Sys.Date functionality or we can use the lubridate package but there is also clock which offers a “tidy” way of managing dates and times.\n\n#make a smaller dataset to work with\nsub&lt;-df %&gt;% select(DIAG_3_01, ADMIDATE, DISDATE,PSEUDO_HESID,MYDOB, SEX)\n\n#calculate number of days someone was admitted\nsub&lt;-sub %&gt;% mutate(duration=as.numeric(DISDATE-ADMIDATE))\n\n#how many different diagnoses do we have in the dataset?\nsub %&gt;% summarise(unique_count = n_distinct(DIAG_3_01))\n \n#which diagnosis is most frequent?\nsub %&gt;% group_by(DIAG_3_01) %&gt;% summarize(count=n()) %&gt;% arrange(desc(count))\n\n#create a histogram of the diagnoses\nsub %&gt;% ggplot(aes(DIAG_3_01)) + geom_bar()\nsub %&gt;% group_by(DIAG_3_01) %&gt;% \n  summarize(diag_count=n()) %&gt;% \n  ggplot(aes(diag_count)) + geom_histogram()\n\n#create a histogram of the duration of admission\nsub %&gt;% group_by(duration) %&gt;% summarize(duration_count=n()) %&gt;% \n  ggplot(aes(duration_count)) + geom_histogram(binwidth=2) + xlim(0,50)\n\nsub %&gt;% group_by(duration) %&gt;% summarize(duration_count=n()) \n\n#how can you calculate the age at admission using the month and year DOB? Note the latter is in a character format instead of a date format\nlibrary(lubridate)\nsub2 &lt;- sub %&gt;% mutate(birth_date = as_date(MYDOB, format = \"%m%Y\")) %&gt;% \n  mutate(age_admit = ADMIDATE - birth_date) %&gt;% \n  mutate(age_admit_years = as.numeric(age_admit)/365.25)\n\nsub2  %&gt;% ggplot(aes(age_admit_years)) + geom_histogram()\n\nThis data could be used to ask a lot of questions. What kind of hypotheses do you have based on what you have seen so far? Think about what you might like to research using AI methods. We will continue with this dataset in the next two activities.",
    "crumbs": [
      "Home",
      "Activities",
      "Activity 1"
    ]
  },
  {
    "objectID": "activities/Activity1.html",
    "href": "activities/Activity1.html",
    "title": "Activity 1",
    "section": "",
    "text": "Pair programming is a technique where two programmers work together at one workstation. One programmer, the driver, writes the code, while the other, the navigator, reviews the code as it’s being written and offers suggestions. The goal is to learn from each other. It’s best practice to switch driver and navigator roles roughly every 10 minutes or whenever you come to a natural break point.\n\n\n\nFor the hands-on application of what you’ve been learning, we are going to use artificial data sets from the United Kingdom’s National Health Service. These are downloaded from here.\nWe need a data dictionary to understand what the columns represent. We are focusing on the admitted patient care data today.Use the excel file HES+TOS+V2.04 and use the HES APC TOS tab.\nWe are using the sample which is 10,000 rows instead of 1 million rows in the “full” dataset. This is due to time and memory constraints on the lab, but you could absolutely do this type of analysis on large sensitive data like the full sample available.\nFirst, we need to go to Terminal and copy the data from a shared folder to your own folder. Remember to replace $USER with your username. cp -r /mnt/work/data/artificial_hes_apc_202302_v1_sample/ /mnt/work/workbench/$USER/\n\n##call libraries\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\n#remember to replace $USER\n\ndata_dir&lt;-\"/mnt/work/workbench/$USER/artificial_hes_apc_202302_v1_sample/artificial_hes_apc_0102.csv\"\n\n\n#read in .csv\ndf&lt;-read_csv(data_dir)\n\nglimpse(df)\n\nLet’s focus on the diagnoses (DIAG_3_01) the admission date (ADMIDATE); the discharge date (DISDATE); Date of Birth (MYDOB); and sex (SEX). What are the first steps we would take if we want to study what diagnoses are associated with a longer hospital admission time?\nThe tibble shows the dates in a “Date” class. To manipualte dates we can use the Sys.Date functionality or we can use the lubridate package but there is also clock which offers a “tidy” way of managing dates and times.\n\n#make a smaller dataset to work with\nsub&lt;-df %&gt;% select(DIAG_3_01, ADMIDATE, DISDATE,PSEUDO_HESID,MYDOB, SEX)\n\n#calculate number of days someone was admitted\nsub&lt;-sub %&gt;% mutate(duration=as.numeric(DISDATE-ADMIDATE))\n\n#how many different diagnoses do we have in the dataset?\nsub %&gt;% summarise(unique_count = n_distinct(DIAG_3_01))\n \n#which diagnosis is most frequent?\nsub %&gt;% group_by(DIAG_3_01) %&gt;% summarize(count=n())\n\n#create a histogram of the diagnoses\n\n#create a histogram of the duration of admission\n\n#how can you calculate the age at admission using the month and year DOB? Note the latter is in a character format instead of a date format\n\nThis data could be used to ask a lot of questions. What kind of hypotheses do you have based on what you have seen so far? Think about what you might like to research using AI methods. We will continue with this dataset in the next two activities.",
    "crumbs": [
      "Home",
      "Activities",
      "Activity 1"
    ]
  },
  {
    "objectID": "activities/Activity1.html#activity-1",
    "href": "activities/Activity1.html#activity-1",
    "title": "Activity 1",
    "section": "",
    "text": "Pair programming is a technique where two programmers work together at one workstation. One programmer, the driver, writes the code, while the other, the navigator, reviews the code as it’s being written and offers suggestions. The goal is to learn from each other. It’s best practice to switch driver and navigator roles roughly every 10 minutes or whenever you come to a natural break point.\n\n\n\nFor the hands-on application of what you’ve been learning, we are going to use artificial data sets from the United Kingdom’s National Health Service. These are downloaded from here.\nWe need a data dictionary to understand what the columns represent. We are focusing on the admitted patient care data today.Use the excel file HES+TOS+V2.04 and use the HES APC TOS tab.\nWe are using the sample which is 10,000 rows instead of 1 million rows in the “full” dataset. This is due to time and memory constraints on the lab, but you could absolutely do this type of analysis on large sensitive data like the full sample available.\nFirst, we need to go to Terminal and copy the data from a shared folder to your own folder. Remember to replace $USER with your username. cp -r /mnt/work/data/artificial_hes_apc_202302_v1_sample/ /mnt/work/workbench/$USER/\n\n##call libraries\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\n\n#remember to replace $USER\n\ndata_dir&lt;-\"/mnt/work/workbench/$USER/artificial_hes_apc_202302_v1_sample/artificial_hes_apc_0102.csv\"\n\n\n#read in .csv\ndf&lt;-read_csv(data_dir)\n\nglimpse(df)\n\nLet’s focus on the diagnoses (DIAG_3_01) the admission date (ADMIDATE); the discharge date (DISDATE); Date of Birth (MYDOB); and sex (SEX). What are the first steps we would take if we want to study what diagnoses are associated with a longer hospital admission time?\nThe tibble shows the dates in a “Date” class. To manipualte dates we can use the Sys.Date functionality or we can use the lubridate package but there is also clock which offers a “tidy” way of managing dates and times.\n\n#make a smaller dataset to work with\nsub&lt;-df %&gt;% select(DIAG_3_01, ADMIDATE, DISDATE,PSEUDO_HESID,MYDOB, SEX)\n\n#calculate number of days someone was admitted\nsub&lt;-sub %&gt;% mutate(duration=as.numeric(DISDATE-ADMIDATE))\n\n#how many different diagnoses do we have in the dataset?\nsub %&gt;% summarise(unique_count = n_distinct(DIAG_3_01))\n \n#which diagnosis is most frequent?\nsub %&gt;% group_by(DIAG_3_01) %&gt;% summarize(count=n())\n\n#create a histogram of the diagnoses\n\n#create a histogram of the duration of admission\n\n#how can you calculate the age at admission using the month and year DOB? Note the latter is in a character format instead of a date format\n\nThis data could be used to ask a lot of questions. What kind of hypotheses do you have based on what you have seen so far? Think about what you might like to research using AI methods. We will continue with this dataset in the next two activities.",
    "crumbs": [
      "Home",
      "Activities",
      "Activity 1"
    ]
  },
  {
    "objectID": "activities/Activity2.html",
    "href": "activities/Activity2.html",
    "title": "Activity2",
    "section": "",
    "text": "Use information from previous modules to start analyzing a new large dataset. We’ve downloaded Artificial data from the UK National Healthcare System from this website\nYour objective is to perform exploratory data analysis with the artificial data, which is very like real sensitive data, and use some classic statistical methods.\n\n##call libraries\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\n#remember to make this path make sense efor the working directory you are currently in\ndata_dir&lt;-\"artificial_hes_apc_0102.csv\"\n\n#read in .csv\ndf&lt;-read_csv(data_dir)\n\nglimpse(df)\n\n#make a smaller dataset to work with\nsub&lt;-df %&gt;% select(DIAG_3_01, ADMIDATE, DISDATE,PSEUDO_HESID,MYDOB, SEX)\n\n#calculate number of days someone was admitted\nsub&lt;-sub %&gt;% \n  mutate(duration=as.numeric(DISDATE-ADMIDATE))\n\n#### pre processing\n\n\n#### use lm() to perform linear regression\n\n#### use tidymodels to perform linear regression",
    "crumbs": [
      "Home",
      "Activities",
      "Activity2"
    ]
  },
  {
    "objectID": "activities/Activity2.html#activity-2",
    "href": "activities/Activity2.html#activity-2",
    "title": "Activity2",
    "section": "",
    "text": "Use information from previous modules to start analyzing a new large dataset. We’ve downloaded Artificial data from the UK National Healthcare System from this website\nYour objective is to perform exploratory data analysis with the artificial data, which is very like real sensitive data, and use some classic statistical methods.\n\n##call libraries\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\n#remember to make this path make sense efor the working directory you are currently in\ndata_dir&lt;-\"artificial_hes_apc_0102.csv\"\n\n#read in .csv\ndf&lt;-read_csv(data_dir)\n\nglimpse(df)\n\n#make a smaller dataset to work with\nsub&lt;-df %&gt;% select(DIAG_3_01, ADMIDATE, DISDATE,PSEUDO_HESID,MYDOB, SEX)\n\n#calculate number of days someone was admitted\nsub&lt;-sub %&gt;% \n  mutate(duration=as.numeric(DISDATE-ADMIDATE))\n\n#### pre processing\n\n\n#### use lm() to perform linear regression\n\n#### use tidymodels to perform linear regression",
    "crumbs": [
      "Home",
      "Activities",
      "Activity2"
    ]
  },
  {
    "objectID": "activities/Activity3.html",
    "href": "activities/Activity3.html",
    "title": "Activity2",
    "section": "",
    "text": "Use information from previous modules to perform ML/AI.\nYour goal is the following: * Choose a question you’re interested in * Build a predictive model (e.g., classification or regression); * Evaluate model performance * Create visualization * Interpret the model and findings\n\n##call libraries\nlibrary(c(dplyr,tidyr,readr,ggplot2))\n\n#remember to replace $USER\n\n#set directory path\ndata_dir&lt;-\"/mnt/work/workbench/$USER/artificial_hes_apc_202302_v1_sample/artificial_hes_apc_0102.csv\"\n\n#read in .csv\ndf&lt;-read_csv(data_dir)\n\nglimpse(df)\n\n#make a smaller dataset to work with; for example:\nsub&lt;-df %&gt;% select(DIAG_3_01, ADMIDATE, DISDATE,PSEUDO_HESID,MYDOB, SEX)\n\nglimpse(sub)",
    "crumbs": [
      "Home",
      "Activities",
      "Activity2"
    ]
  },
  {
    "objectID": "activities/Activity3.html#activity-3",
    "href": "activities/Activity3.html#activity-3",
    "title": "Activity2",
    "section": "",
    "text": "Use information from previous modules to perform ML/AI.\nYour goal is the following: * Choose a question you’re interested in * Build a predictive model (e.g., classification or regression); * Evaluate model performance * Create visualization * Interpret the model and findings\n\n##call libraries\nlibrary(c(dplyr,tidyr,readr,ggplot2))\n\n#remember to replace $USER\n\n#set directory path\ndata_dir&lt;-\"/mnt/work/workbench/$USER/artificial_hes_apc_202302_v1_sample/artificial_hes_apc_0102.csv\"\n\n#read in .csv\ndf&lt;-read_csv(data_dir)\n\nglimpse(df)\n\n#make a smaller dataset to work with; for example:\nsub&lt;-df %&gt;% select(DIAG_3_01, ADMIDATE, DISDATE,PSEUDO_HESID,MYDOB, SEX)\n\nglimpse(sub)",
    "crumbs": [
      "Home",
      "Activities",
      "Activity2"
    ]
  },
  {
    "objectID": "modules/Module1.html",
    "href": "modules/Module1.html",
    "title": "Module 1",
    "section": "",
    "text": "Content for this Module is derived from * https://bookdown.org/rdpeng/rprogdatascience/r-nuts-and-bolts.html * https://hbctraining.github.io/Intro-to-R-flipped/lessons/04_introR_packages.html * https://swcarpentry.github.io/r-novice-inflammation/01-starting-with-data.html#manipulating-data\nChatGPT was used in the creation of examples and explanations.\n\n\n\nBy the end of this session, learners will be able to: * Basics of R programming * Understanding the RStudio IDE * RMarkdown for reproducible research\n\n\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\nA script ends in .R) and a notebook ends in .Rmd\n\n\n\nR has five basic or “atomic” classes of objects: - character - numeric (real numbers) - integer - complex - logical (True/False)\nOnce a variable is created, we can use the variable name to refer to the value it was assigned. The variable name now acts as a tag. To see the value of a variable, we can print it by: 1) typing the name of the variable and hitting return in the Console 2) if your cursor is on a line in the editor window, you can use control-enter (PC) or command-return (Mac) 3) press the Run button above 4) press the green play button in the RMarkdown code chunk In general, R will print to the console any object returned by a function or operation unless we assign it to a variable.\n\n#we use an arrow to assign a value to a variable\nx&lt;-5\nclass(x)\n\n#we can coerce an object from one class to another\ny&lt;-as.integer(x)\nclass(y)\n\nz&lt;-1L\nclass(z)\n\nIt is important that we name variables in a way that other users and our future selves will understand. Today, most R programmers 1) start variable names with lower case letters, 2) separate words in variable names with underscores, and 3) use only lowercase letters, underscores, and numbers in variable names. The Tidyverse Style Guide includes a section on this.\nIt’s also important to use the # hash symbol to create comments for other users and our future selves.\nThe most basic type of R object is a vector. Vectors can only contain objects of the same class. A list is represented as a vector but can contain objects of different classes.\n\n#initialize a vector\na&lt;-vector(\"numeric\",10)\nprint(a)\n\n#define a vector with c() function\nb&lt;-c(1,2,3)\nprint(b)\n\n#check what class the variables are\nclass(a)\nclass(b)\n\n#create a data frame\nx&lt;-1:10\ny&lt;-rnorm(10)\ndf&lt;-data.frame(x,y)\n\n\n\n\n\n#as a calculator\n1+2\nsqrt(25)\n\n\n# View summary statistics\nheights &lt;- c(150, 160, 165, 170, 155)\nmean(heights)\nsummary(heights)\n\n# plot data using base R\nplot(heights)\n\n# perform statistical tests\nheights2&lt;-c(100,80,85,90)\nt.test(heights,heights2)\n\n\n\n\nPackages are collections of R functions, data, and compiled code in a well-defined format, created to add specific functionality. You can do a lot with base R, but there are 10,000+ user contributed packages on CRAN. The directories in R where the packages are stored are called the libraries.\nIf you’re using R on your computer, you can use the function install.packages to install a package. You can also use RStudio. However, because of the way workbench is set up in R, we need to use the Terminal to install packages. Return to the JupyterLab tab and select Terminal from the Launcher.\nExecute the following commands: conda activate r-base conda install r-ggplot\n\n#what libraries are loaded in the environment?\nsessionInfo()\n\n#now we call the package we installed\nlibrary(ggplot2)\n\n#now we can use functions from the ggplot2 package\nggplot(df,aes(x,y)) + geom_point()\n\nObject-oriented programming is a programming paradigm based on the concept of breaking down a problem or system into objects.\nFunctional programming is a declarative programming paradigm style where one applies pure functions in sequence to solve complex problems. Functions take an input value and produce an output value without being affected by the program.\nR lends itself well to a style of problem solving centered on functions so it can be used as a functional language. However, object orienting programm is also possible in R, using the S3 system for example. These are more advanced computer science concepts which aren’t required to understand, but are good to be aware of.\nHUNT Cloud is a secure environment for analyzing sensitive data.\nUsing sensitive data in biomedical research requires a careful balance between scientific advancement and the protection of individual privacy and rights. Sensitive data often includes personally identifiable information (PII) such as names, dates of birth, or contact details, as well as health-related data like genetic information, diagnoses, test results, or treatment records. Because this type of data can reveal intimate details about a person’s health status and identity, it is subject to strict ethical, legal, and institutional safeguards.\nResearchers must adhere to national and international regulations, such as the General Data Protection Regulation (GDPR) in the European Union. These frameworks define how sensitive data can be collected, stored, shared, and used. For example, under GDPR, data must be processed lawfully, transparently, and only for specified purposes. Researchers are also required to implement data minimization and to ensure that data are accurate, kept up to date, and stored securely.\nIn practice, ethical research with sensitive data involves several layers of protection. First, researchers typically seek approval from an ethics committee, which assesses the risks and benefits of the study and ensures that informed consent is obtained. Consent forms should clearly explain how the data will be used, who will have access, and what measures are in place to protect confidentiality. In some cases, broad consent may be used for biobanks or longitudinal studies like the HUNT Study, but this must still respect participants’ autonomy and rights.\nTo further protect individuals, sensitive data are often de-identified or pseudonymized, meaning that direct identifiers are removed or replaced with codes. However, even de-identified data can carry risks of re-identification, particularly when linked with other datasets. As a result, researchers must employ technical safeguards (such as encryption and secure servers) as well as administrative controls (like restricted access and data-use agreements) to mitigate these risks.\nUltimately, the responsible use of sensitive data is essential for building public trust and ensuring the long-term sustainability of biomedical research. Using sensitive data in biomedical research requires a careful balance between scientific advancement and the protection of individual privacy and rights. Sensitive data often includes personally identifiable information (PII) such as names, dates of birth, or contact details, as well as health-related data like genetic information, diagnoses, test results, or treatment records. Because this type of data can reveal intimate details about a person’s health status and identity, it is subject to strict ethical, legal, and institutional safeguards.\nResearchers must adhere to national and international regulations, such as the General Data Protection Regulation (GDPR) in the European Union or the Health Insurance Portability and Accountability Act (HIPAA) in the United States. These frameworks define how sensitive data can be collected, stored, shared, and used. For example, under GDPR, data must be processed lawfully, transparently, and only for specified purposes. Researchers are also required to implement data minimization and to ensure that data are accurate, kept up to date, and stored securely.\nIn practice, ethical research with sensitive data involves several layers of protection. First, researchers typically seek approval from an ethics committee or institutional review board (IRB), which assesses the risks and benefits of the study and ensures that informed consent is obtained. Consent forms should clearly explain how the data will be used, who will have access, and what measures are in place to protect confidentiality. In some cases, broad consent may be used for biobanks or longitudinal studies, but this must still respect participants’ autonomy and rights.\nTo further protect individuals, sensitive data are often de-identified or pseudonymized, meaning that direct identifiers are removed or replaced with codes. However, even de-identified data can carry risks of re-identification, particularly when linked with other datasets. As a result, researchers must employ technical safeguards (such as encryption and secure servers) as well as administrative controls (like restricted access and data-use agreements) to mitigate these risks.\nUltimately, the responsible use of sensitive data is essential for building public trust and ensuring the long-term sustainability of biomedical research.\nRemember * Never store sensitive data in plaintext without protection. * Always store sensitive data outside version control systems like Git. Use .ignore files to achieve this.",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module 1"
    ]
  },
  {
    "objectID": "modules/Module1.html#module-1-introduction-to-r",
    "href": "modules/Module1.html#module-1-introduction-to-r",
    "title": "Module 1",
    "section": "",
    "text": "Content for this Module is derived from * https://bookdown.org/rdpeng/rprogdatascience/r-nuts-and-bolts.html * https://hbctraining.github.io/Intro-to-R-flipped/lessons/04_introR_packages.html * https://swcarpentry.github.io/r-novice-inflammation/01-starting-with-data.html#manipulating-data\nChatGPT was used in the creation of examples and explanations.\n\n\n\nBy the end of this session, learners will be able to: * Basics of R programming * Understanding the RStudio IDE * RMarkdown for reproducible research\n\n\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\nA script ends in .R) and a notebook ends in .Rmd\n\n\n\nR has five basic or “atomic” classes of objects: - character - numeric (real numbers) - integer - complex - logical (True/False)\nOnce a variable is created, we can use the variable name to refer to the value it was assigned. The variable name now acts as a tag. To see the value of a variable, we can print it by: 1) typing the name of the variable and hitting return in the Console 2) if your cursor is on a line in the editor window, you can use control-enter (PC) or command-return (Mac) 3) press the Run button above 4) press the green play button in the RMarkdown code chunk In general, R will print to the console any object returned by a function or operation unless we assign it to a variable.\n\n#we use an arrow to assign a value to a variable\nx&lt;-5\nclass(x)\n\n#we can coerce an object from one class to another\ny&lt;-as.integer(x)\nclass(y)\n\nz&lt;-1L\nclass(z)\n\nIt is important that we name variables in a way that other users and our future selves will understand. Today, most R programmers 1) start variable names with lower case letters, 2) separate words in variable names with underscores, and 3) use only lowercase letters, underscores, and numbers in variable names. The Tidyverse Style Guide includes a section on this.\nIt’s also important to use the # hash symbol to create comments for other users and our future selves.\nThe most basic type of R object is a vector. Vectors can only contain objects of the same class. A list is represented as a vector but can contain objects of different classes.\n\n#initialize a vector\na&lt;-vector(\"numeric\",10)\nprint(a)\n\n#define a vector with c() function\nb&lt;-c(1,2,3)\nprint(b)\n\n#check what class the variables are\nclass(a)\nclass(b)\n\n#create a data frame\nx&lt;-1:10\ny&lt;-rnorm(10)\ndf&lt;-data.frame(x,y)\n\n\n\n\n\n#as a calculator\n1+2\nsqrt(25)\n\n\n# View summary statistics\nheights &lt;- c(150, 160, 165, 170, 155)\nmean(heights)\nsummary(heights)\n\n# plot data using base R\nplot(heights)\n\n# perform statistical tests\nheights2&lt;-c(100,80,85,90)\nt.test(heights,heights2)\n\n\n\n\nPackages are collections of R functions, data, and compiled code in a well-defined format, created to add specific functionality. You can do a lot with base R, but there are 10,000+ user contributed packages on CRAN. The directories in R where the packages are stored are called the libraries.\nIf you’re using R on your computer, you can use the function install.packages to install a package. You can also use RStudio. However, because of the way workbench is set up in R, we need to use the Terminal to install packages. Return to the JupyterLab tab and select Terminal from the Launcher.\nExecute the following commands: conda activate r-base conda install r-ggplot\n\n#what libraries are loaded in the environment?\nsessionInfo()\n\n#now we call the package we installed\nlibrary(ggplot2)\n\n#now we can use functions from the ggplot2 package\nggplot(df,aes(x,y)) + geom_point()\n\nObject-oriented programming is a programming paradigm based on the concept of breaking down a problem or system into objects.\nFunctional programming is a declarative programming paradigm style where one applies pure functions in sequence to solve complex problems. Functions take an input value and produce an output value without being affected by the program.\nR lends itself well to a style of problem solving centered on functions so it can be used as a functional language. However, object orienting programm is also possible in R, using the S3 system for example. These are more advanced computer science concepts which aren’t required to understand, but are good to be aware of.\nHUNT Cloud is a secure environment for analyzing sensitive data.\nUsing sensitive data in biomedical research requires a careful balance between scientific advancement and the protection of individual privacy and rights. Sensitive data often includes personally identifiable information (PII) such as names, dates of birth, or contact details, as well as health-related data like genetic information, diagnoses, test results, or treatment records. Because this type of data can reveal intimate details about a person’s health status and identity, it is subject to strict ethical, legal, and institutional safeguards.\nResearchers must adhere to national and international regulations, such as the General Data Protection Regulation (GDPR) in the European Union. These frameworks define how sensitive data can be collected, stored, shared, and used. For example, under GDPR, data must be processed lawfully, transparently, and only for specified purposes. Researchers are also required to implement data minimization and to ensure that data are accurate, kept up to date, and stored securely.\nIn practice, ethical research with sensitive data involves several layers of protection. First, researchers typically seek approval from an ethics committee, which assesses the risks and benefits of the study and ensures that informed consent is obtained. Consent forms should clearly explain how the data will be used, who will have access, and what measures are in place to protect confidentiality. In some cases, broad consent may be used for biobanks or longitudinal studies like the HUNT Study, but this must still respect participants’ autonomy and rights.\nTo further protect individuals, sensitive data are often de-identified or pseudonymized, meaning that direct identifiers are removed or replaced with codes. However, even de-identified data can carry risks of re-identification, particularly when linked with other datasets. As a result, researchers must employ technical safeguards (such as encryption and secure servers) as well as administrative controls (like restricted access and data-use agreements) to mitigate these risks.\nUltimately, the responsible use of sensitive data is essential for building public trust and ensuring the long-term sustainability of biomedical research. Using sensitive data in biomedical research requires a careful balance between scientific advancement and the protection of individual privacy and rights. Sensitive data often includes personally identifiable information (PII) such as names, dates of birth, or contact details, as well as health-related data like genetic information, diagnoses, test results, or treatment records. Because this type of data can reveal intimate details about a person’s health status and identity, it is subject to strict ethical, legal, and institutional safeguards.\nResearchers must adhere to national and international regulations, such as the General Data Protection Regulation (GDPR) in the European Union or the Health Insurance Portability and Accountability Act (HIPAA) in the United States. These frameworks define how sensitive data can be collected, stored, shared, and used. For example, under GDPR, data must be processed lawfully, transparently, and only for specified purposes. Researchers are also required to implement data minimization and to ensure that data are accurate, kept up to date, and stored securely.\nIn practice, ethical research with sensitive data involves several layers of protection. First, researchers typically seek approval from an ethics committee or institutional review board (IRB), which assesses the risks and benefits of the study and ensures that informed consent is obtained. Consent forms should clearly explain how the data will be used, who will have access, and what measures are in place to protect confidentiality. In some cases, broad consent may be used for biobanks or longitudinal studies, but this must still respect participants’ autonomy and rights.\nTo further protect individuals, sensitive data are often de-identified or pseudonymized, meaning that direct identifiers are removed or replaced with codes. However, even de-identified data can carry risks of re-identification, particularly when linked with other datasets. As a result, researchers must employ technical safeguards (such as encryption and secure servers) as well as administrative controls (like restricted access and data-use agreements) to mitigate these risks.\nUltimately, the responsible use of sensitive data is essential for building public trust and ensuring the long-term sustainability of biomedical research.\nRemember * Never store sensitive data in plaintext without protection. * Always store sensitive data outside version control systems like Git. Use .ignore files to achieve this.",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module 1"
    ]
  },
  {
    "objectID": "modules/Module2_working.html",
    "href": "modules/Module2_working.html",
    "title": "Module 2 Blanks",
    "section": "",
    "text": "This lesson is based on content from [Software Carpentry * https://swcarpentry.github.io/r-novice-inflammation/01-starting-with-data.html * https://datacarpentry.github.io/genomics-r-intro/05-dplyr.html * https://datacarpentry.github.io/r-socialsci/03-dplyr.html * https://datacarpentry.github.io/r-socialsci/04-tidyr.html * https://mqwilber.github.io/2015-04-17-ucsb/lessons/intro_R_lessons/R_data_plotting_analysis.html ChatGPT was used in the creation of examples and explanations.\n\n\nBy the end of this session, learners will be able to: * Download data in Terminal to use in RStudio * Differentiate between data frames, lists, and matrices * Use base R to subset data * Use the dplyr package to manipulate dataframes. * Use select() to choose variables from a dataframe. * Use filter() to choose data based on values. * Use group_by() and summarize() to work with subsets of data. *Use mutate() to create new variables.\n\n\n\n* Data frame: most common tabular structure in R\n* List: collection of elements of different types\n* Matrix: 2D array of only one data type\n\n# Data frame\ndf &lt;- data.frame(\n  ID = 1:3,\n  Name = c(\"Alice\", \"Bob\", \"Clara\"),\n  Age = c(34, 45, 29)\n)\n\n# List\npatient &lt;- list(\n  ID = 101,\n  Name = \"David\",\n  Vitals = c(BP = 120, HR = 70),\n  Notes = c(\"No allergies\", \"Nonsmoker\")\n)\n\n# Matrix\nmat &lt;- matrix(1:9, nrow = 3, byrow = TRUE)\n\n# Explore structure\n\n\n\n\nFirst, we need to load in the libraries we will use. Remember to go to Terminal to install new packages.\nconda install r-dplyr r-tidyr r-readr\n\n#library(tidyverse)\n#The tidyverse package is an “umbrella-package” that installs several packages useful for data analysis which work together well such as tidyr, dplyr, readr, ggplot2, tibble, etc. It can take some time to install, so do that later if you would like.\n\nWe are going to download example data from Software Carpentry.\nIn Terminal, navigate to your directory. cd /mnt/work/workbench/&lt;username&gt;/\nExecute the commands to download from a URL wget https://swcarpentry.github.io/r-novice-inflammation/data/r-novice-inflammation-data.zip\nUnzip the data unzip r-novice-inflammation-data.zip\nChange directory into the new folder cd data\nList everything in the directory ls\nNow, let’s read our data into R.\nI like to use the data.table package which has a function fread() and can be memory efficient for reading larger datasets. This creates a data.table object which has its own grammar for manipulating.\nThe tidyverse has the readr package with the function read_csv(). This creates a tibble.\nbase R has the function read.csv() which creates a basic data frame.\n\nusername&lt;-\"YOUR USERNAME HERE\"\npath&lt;-paste0(\"/mnt/work/workbench/\",username,\"/scratch/\",username,\"/data/\")\npath2&lt;-paste0(path,\"sample.csv\")\n\n#we pass 2 arguments, the path where the file is, and information about if the file has column names\n#we assign the data frame information to the variable \"dat\"\n\n\n#what do you think the col_names option does? how can we find out? what is the default parameter for header? what would happen if we did not use it?\n\n\n\n#let's use base R because it's a little easier for beginners\n\nUsing head we see that R has auto-generated column names in the sequence V1 (for “variable 1”), V2, and so on, until V40. We will also inspect our data to get a sense of what it looks like.\nEach row holds the observations for just one patient. Each column holds values for several variables such as gender, blood pressure, and age.\nFor every column in the data frame, the function summary calculates: the minimum value, the first quartile, the median, the mean, the third quartile and the max value, giving helpful details about the sample distribution.\nLike many programming languages, we can use indices to specify a certain row and column. The first value in brackets is row, and the second is column. R starts indices at 1, but some other langauges use 0, so be mindful of this.\nIf we want to select more than one row or column, we can use the function c, which combines the values you give it into one vector or list.\nIf we want to select contiguous rows or columns, we can use the colon.\n\n#this will pull out the value in the 30th row and 20th column\n\n\n#let's check what 1:5 does\n\n\n#now let's use it in row and column space\n\nIf we leave the row or column space open, it means we want to select all of that row or column.\n\n#All columns from row 3 to see the values for Sub003\n\n\n# All rows from column 6-9 regarding Anuerysms\n\n\n#what do you think happens if we leave both values empty\n\nWe can also use column names. This is helpful because if you add columns or rows later, the indices could change.\n\n#using the $ operator \n\n\n#Name within square brackets\n\n\n#rows can have names too; in this case they're just the row index\n\nAll the indexing and subsetting that works on data frames also works on vectors.\n\n#pull out a column from the data frame as a vector\n\n\n#subset for the first 10 participants\n\n\n\n\nCommon dplyr verbs: filter, select, mutate, summarize, group_by Check out the dplyr cheatsheet\nNote: The packages in the tidyverse, namely dplyr, tidyr and ggplot2 accept both the British (e.g. summarise) and American (e.g. summarize) spelling variants of different function and option names.\n\n\n\n#functionality similar to str()\n\n\n#in dplyr we can select the columns by name\n\n\n#another way to do this is with the pipe\n\n\n# we can use filter to filter observations for individuals with high blood pressure, this time let's save it into a new data frame\n\n\n# what object class is high\n\nYou may also have noticed that the output from the object high doesn’t run off the screen anymore. It’s one of the advantages of tbl_df (also called tibble), the central data class in the tidyverse, compared to normal dataframes in R.\n\n#you can see we went from 100 to 30 observations when we used filter()\n\n\n# We can also specify multiple conditions within the filter() function.\n\n#high blood pressure and the control group\n\n# To form “and” statements within dplyr, we can pass our desired conditions as arguments in the filter() function, separated by commas:\n\n\n# or we can use the ampersand \"&\" operator\n\n\n#how can we check that both of these filtered data frames are the same size?\n\n# Have you noticed that the Gender column uses two conventions for male/female (lower case and upper case)?\n\n\n# To filter for the male participants, we will need to accomodate both conventions\n\n# For an “or” statement, observations must meet at least one of the specified conditions. To form “or” statements we use the logical operator for “or,” which is the vertical bar (|):",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module 2 Blanks"
    ]
  },
  {
    "objectID": "modules/Module2_working.html#module-2-data-handling-in-r",
    "href": "modules/Module2_working.html#module-2-data-handling-in-r",
    "title": "Module 2 Blanks",
    "section": "",
    "text": "This lesson is based on content from [Software Carpentry * https://swcarpentry.github.io/r-novice-inflammation/01-starting-with-data.html * https://datacarpentry.github.io/genomics-r-intro/05-dplyr.html * https://datacarpentry.github.io/r-socialsci/03-dplyr.html * https://datacarpentry.github.io/r-socialsci/04-tidyr.html * https://mqwilber.github.io/2015-04-17-ucsb/lessons/intro_R_lessons/R_data_plotting_analysis.html ChatGPT was used in the creation of examples and explanations.\n\n\nBy the end of this session, learners will be able to: * Download data in Terminal to use in RStudio * Differentiate between data frames, lists, and matrices * Use base R to subset data * Use the dplyr package to manipulate dataframes. * Use select() to choose variables from a dataframe. * Use filter() to choose data based on values. * Use group_by() and summarize() to work with subsets of data. *Use mutate() to create new variables.\n\n\n\n* Data frame: most common tabular structure in R\n* List: collection of elements of different types\n* Matrix: 2D array of only one data type\n\n# Data frame\ndf &lt;- data.frame(\n  ID = 1:3,\n  Name = c(\"Alice\", \"Bob\", \"Clara\"),\n  Age = c(34, 45, 29)\n)\n\n# List\npatient &lt;- list(\n  ID = 101,\n  Name = \"David\",\n  Vitals = c(BP = 120, HR = 70),\n  Notes = c(\"No allergies\", \"Nonsmoker\")\n)\n\n# Matrix\nmat &lt;- matrix(1:9, nrow = 3, byrow = TRUE)\n\n# Explore structure\n\n\n\n\nFirst, we need to load in the libraries we will use. Remember to go to Terminal to install new packages.\nconda install r-dplyr r-tidyr r-readr\n\n#library(tidyverse)\n#The tidyverse package is an “umbrella-package” that installs several packages useful for data analysis which work together well such as tidyr, dplyr, readr, ggplot2, tibble, etc. It can take some time to install, so do that later if you would like.\n\nWe are going to download example data from Software Carpentry.\nIn Terminal, navigate to your directory. cd /mnt/work/workbench/&lt;username&gt;/\nExecute the commands to download from a URL wget https://swcarpentry.github.io/r-novice-inflammation/data/r-novice-inflammation-data.zip\nUnzip the data unzip r-novice-inflammation-data.zip\nChange directory into the new folder cd data\nList everything in the directory ls\nNow, let’s read our data into R.\nI like to use the data.table package which has a function fread() and can be memory efficient for reading larger datasets. This creates a data.table object which has its own grammar for manipulating.\nThe tidyverse has the readr package with the function read_csv(). This creates a tibble.\nbase R has the function read.csv() which creates a basic data frame.\n\nusername&lt;-\"YOUR USERNAME HERE\"\npath&lt;-paste0(\"/mnt/work/workbench/\",username,\"/scratch/\",username,\"/data/\")\npath2&lt;-paste0(path,\"sample.csv\")\n\n#we pass 2 arguments, the path where the file is, and information about if the file has column names\n#we assign the data frame information to the variable \"dat\"\n\n\n#what do you think the col_names option does? how can we find out? what is the default parameter for header? what would happen if we did not use it?\n\n\n\n#let's use base R because it's a little easier for beginners\n\nUsing head we see that R has auto-generated column names in the sequence V1 (for “variable 1”), V2, and so on, until V40. We will also inspect our data to get a sense of what it looks like.\nEach row holds the observations for just one patient. Each column holds values for several variables such as gender, blood pressure, and age.\nFor every column in the data frame, the function summary calculates: the minimum value, the first quartile, the median, the mean, the third quartile and the max value, giving helpful details about the sample distribution.\nLike many programming languages, we can use indices to specify a certain row and column. The first value in brackets is row, and the second is column. R starts indices at 1, but some other langauges use 0, so be mindful of this.\nIf we want to select more than one row or column, we can use the function c, which combines the values you give it into one vector or list.\nIf we want to select contiguous rows or columns, we can use the colon.\n\n#this will pull out the value in the 30th row and 20th column\n\n\n#let's check what 1:5 does\n\n\n#now let's use it in row and column space\n\nIf we leave the row or column space open, it means we want to select all of that row or column.\n\n#All columns from row 3 to see the values for Sub003\n\n\n# All rows from column 6-9 regarding Anuerysms\n\n\n#what do you think happens if we leave both values empty\n\nWe can also use column names. This is helpful because if you add columns or rows later, the indices could change.\n\n#using the $ operator \n\n\n#Name within square brackets\n\n\n#rows can have names too; in this case they're just the row index\n\nAll the indexing and subsetting that works on data frames also works on vectors.\n\n#pull out a column from the data frame as a vector\n\n\n#subset for the first 10 participants\n\n\n\n\nCommon dplyr verbs: filter, select, mutate, summarize, group_by Check out the dplyr cheatsheet\nNote: The packages in the tidyverse, namely dplyr, tidyr and ggplot2 accept both the British (e.g. summarise) and American (e.g. summarize) spelling variants of different function and option names.\n\n\n\n#functionality similar to str()\n\n\n#in dplyr we can select the columns by name\n\n\n#another way to do this is with the pipe\n\n\n# we can use filter to filter observations for individuals with high blood pressure, this time let's save it into a new data frame\n\n\n# what object class is high\n\nYou may also have noticed that the output from the object high doesn’t run off the screen anymore. It’s one of the advantages of tbl_df (also called tibble), the central data class in the tidyverse, compared to normal dataframes in R.\n\n#you can see we went from 100 to 30 observations when we used filter()\n\n\n# We can also specify multiple conditions within the filter() function.\n\n#high blood pressure and the control group\n\n# To form “and” statements within dplyr, we can pass our desired conditions as arguments in the filter() function, separated by commas:\n\n\n# or we can use the ampersand \"&\" operator\n\n\n#how can we check that both of these filtered data frames are the same size?\n\n# Have you noticed that the Gender column uses two conventions for male/female (lower case and upper case)?\n\n\n# To filter for the male participants, we will need to accomodate both conventions\n\n# For an “or” statement, observations must meet at least one of the specified conditions. To form “or” statements we use the logical operator for “or,” which is the vertical bar (|):",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module 2 Blanks"
    ]
  },
  {
    "objectID": "modules/Module2.html",
    "href": "modules/Module2.html",
    "title": "Module 2",
    "section": "",
    "text": "This lesson is based on content from Software Carpentry * https://swcarpentry.github.io/r-novice-inflammation/01-starting-with-data.html * https://datacarpentry.github.io/genomics-r-intro/05-dplyr.html * https://datacarpentry.github.io/r-socialsci/03-dplyr.html * https://datacarpentry.github.io/r-socialsci/04-tidyr.html * https://mqwilber.github.io/2015-04-17-ucsb/lessons/intro_R_lessons/R_data_plotting_analysis.html\n\nChatGPT was used in the creation of examples and explanations.\n\n\n\nBy the end of this session, learners will be able to: * Download data in Terminal to use in RStudio * Differentiate between data frames, lists, and matrices * Use base R to subset data * Use the dplyr package to manipulate dataframes. * Use select() to choose variables from a dataframe. * Use filter() to choose data based on values. * Use group_by() and summarize() to work with subsets of data. * Use mutate() to create new variables.\n\n\n\n\nData frame: most common tabular structure in R\nList: collection of elements of different types\nMatrix: 2D array of only one data type\n\n\n# Data frame\ndf &lt;- data.frame(\n  ID = 1:3,\n  Name = c(\"Alice\", \"Bob\", \"Clara\"),\n  Age = c(34, 45, 29)\n)\n\n# List\npatient &lt;- list(\n  ID = 101,\n  Name = \"David\",\n  Vitals = c(BP = 120, HR = 70),\n  Notes = c(\"No allergies\", \"Nonsmoker\")\n)\n\n# Matrix\nmat &lt;- matrix(1:9, nrow = 3, byrow = TRUE)\n\n# Explore structure\nstr(df)\nstr(patient)\nstr(mat)\n\n\n\n\nFirst, we need to load in the libraries we will use. Remember to go to Terminal to install new packages.\nconda install r-dplyr r-tidyr r-readr\n\nThe tidyverse package is an “umbrella-package” that installs several packages useful for data analysis which work together well such as tidyr, dplyr, readr, ggplot2, tibble, etc. It can take some time to install, so do that later if you would like.\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\n\n#library(tidyverse)\n\nWe are going to download example data from Software Carpentry.\nIn Terminal, navigate to your directory. cd /mnt/work/workbench/&lt;username&gt;/\nExecute the commands to download from a URL wget https://swcarpentry.github.io/r-novice-inflammation/data/r-novice-inflammation-data.zip\nUnzip the data unzip r-novice-inflammation-data.zip\nChange directory into the new folder cd data\nList everything in the directory ls\nGo back to the parent directory: cd ..\nSet your working directory in RStudio in the consoles tab: setwd('/mnt/work/workbench/&lt;username&gt;/scratch/&lt;username&gt;')\nSee your current working directory: getwd()\n\n\n\nNavigate to the following URL. The file should download automatically.\nhttps://swcarpentry.github.io/r-novice-inflammation/data/r-novice-inflammation-data.zip\n\nOpen the .zip file. You should see a folder named \"data\".\n\nRight click on the \"data\" folder, and click \"Copy\".\n\nNavigate to the location you have saved the \"HealthAIinR\" folder.\n\nPaste the \"data\" folder to the \"HealthAIinR\" folder.\n\nNow, go back to RStudio. In the consoles tab on the bottom left, enter the following command to check the current working drive.\n`getwd()`\n\nChange the working directory to the \"data\" folder. (Windows users need the slash / in this direction.)\n`setwd('HealthAIinR/')`\nIf you have used GitHub Desktop to clone the repository to your Windows computer, your path will probably be something like: \n\"C:/Users/&lt;username&gt;/OneDrive - &lt;institution&gt;/Documents/GitHub/HealthAIinR\"\nNow, let’s read our data into R. There are several options for reading csv files:\n\nBase R has the function read.csv() which creates a basic data frame.\nI like to use the data.table package which has a function fread() and can be memory efficient for reading larger datasets. This creates a data.table object which has its own grammar for manipulating.\nThe tidyverse has the readr package with the function read_csv(). This creates a tibble, a special type of data frame.\n\n\nusername&lt;-\"YOUR USERNAME HERE\"\npath&lt;-paste0(\"/mnt/work/workbench/\",username,\"/scratch/\",username,\"/data/\")\npath2&lt;-paste0(path,\"sample.csv\")\n\n#we pass 2 arguments, the path where the file is, and information about if the file has column names\n#we assign the data frame information to the variable \"dat\"\ndatt&lt;-read_csv(file=path2,col_names=TRUE)\n    #for local access from Windows computers (given that the working directory is set to \"HealthAIinR/data\") \n        #NB: Cannot use file paths with the read_csv command on Windows, so working directory needs to be set beforehand\n    #datt&lt;-read_csv(\"sample.csv\", col_names=TRUE)\ndatt\nclass(datt)\n\n#what do you think the col_names option does? how can we find out? what is the default parameter for header? what would happen if we did not use it?\n\n?read_csv\n\n#let's use base R because it's a little easier for beginners\ndat&lt;-read.csv(file=path2,header=TRUE)\n    #for local access from Windows computers (given that the working directory is set to \"HealthAIinR/data\") \n        #NB: Cannot use file paths with the read.csv command on Windows, so working directory needs to be set beforehand\n    #dat&lt;-read.csv(\"sample.csv\", header=TRUE)\nhead(dat)\nclass(dat)\n\nUsing head we see that R has auto-generated column names in the sequence V1 (for “variable 1”), V2, and so on, until V40. We will also inspect our data to get a sense of what it looks like.\n\ndim(dat)\n\nnrow(dat)\n\nnames(dat)\n\nstr(dat)\n\nsummary(dat)\n\nEach row holds the observations for just one patient. Each column holds values for several variables such as gender, blood pressure, and age.\nFor every column in the data frame, the function summary calculates: the minimum value, the first quartile, the median, the mean, the third quartile and the max value, giving helpful details about the sample distribution.\nLike many programming languages, we can use indices to specify a certain row and column. The first value in brackets is row, and the second is column. R starts indices at 1, but some other langauges use 0, so be mindful of this.\nIf we want to select more than one row or column, we can use the function c, which combines the values you give it into one vector or list.\nIf we want to select contiguous rows or columns, we can use the colon.\n\n#this will pull out the value in the 30th row and 20th column\ndat[3,5]\n\ndat[c(1, 3, 5), c(2, 4)]\n\n#let's check what 1:5 does\n1:5\n\n#now let's use it in row and column space\ndat[1:5,1:5]\n\nIf we leave the row or column space open, it means we want to select all of that row or column.\n\n#All columns from row 3 to see the values for Sub003\ndat[3,]\n\n# All rows from column 6-9 regarding Anuerysms\ndat[, 6:9]\n\n#what do you think happens if we leave both values empty\ndat[,]\n\nWe can also use column names. This is helpful because if you add columns or rows later, the indices could change.\n\n#using the $ operator \ndat$BloodPressure\n\n#Name within square brackets\ndat[,\"BloodPressure\"]\n\n#rows can have names too; in this case they're just the row index\ndat[\"1\",]\ndat[1,]\n\nAll the indexing and subsetting that works on data frames also works on vectors.\n\n#pull out a column from the data frame as a vector\nBP_vec&lt;-dat$BloodPressure\n\n#subset for the first 10 participants\nBP_vec[1:10]\n\n\n\n\nCommon dplyr verbs: filter, select, mutate, summarize, group_by &gt; Check out the dplyr cheatsheet\nNote: The packages in the tidyverse, namely dplyr, tidyr and ggplot2 accept both the British (e.g. summarise) and American (e.g. summarize) spelling variants of different function and option names.\n\n\n\n#functionality similar to str()\nglimpse(dat)\n\n#in dplyr we can select the columns by name\nselect(dat,Age)\n\n#another way to do this is with the pipe\ndat %&gt;% select(Age)\n\n# we can use filter to filter observations for individuals with high blood pressure, this time let's save it into a new data frame\nhigh &lt;- dat %&gt;% filter(BloodPressure&gt;130)\nhigh\n\n# what object class is high\nclass(high)\n\nYou may also have noticed that the output from the object high doesn’t run off the screen anymore. It’s one of the advantages of tbl_df (also called tibble), the central data class in the tidyverse, compared to normal dataframes in R.\n\n#you can see we went from 100 to 30 observations when we used filter()\nnrow(dat)\nnrow(high)\n\n# We can also specify multiple conditions within the filter() function.\n\n#high blood pressure and the control group\n\n# To form “and” statements within dplyr, we can pass our desired conditions as arguments in the filter() function, separated by commas:\nhigh_control&lt;-dat %&gt;% filter(BloodPressure&gt;130,Group==\"Control\")\n\n# or we can use the ampersand \"&\" operator\nhigh_control&lt;-dat %&gt;% filter(BloodPressure&gt;130 & Group==\"Control\")\n\n#how can we check that both of these filtered data frames are the same size?\n\n# Have you noticed that the Gender column uses two conventions for male/female (lower case and upper case)?\ntable(dat$Gender)\n\n# To filter for the male participants, we will need to accomodate both conventions\n\n# For an “or” statement, observations must meet at least one of the specified conditions. To form “or” statements we use the logical operator for “or,” which is the vertical bar (|):\n\nmales &lt;- dat %&gt;% filter(Gender==\"M\"|Gender==\"m\")\ntable(males$Gender)",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module 2"
    ]
  },
  {
    "objectID": "modules/Module2.html#module-2-data-handling-in-r",
    "href": "modules/Module2.html#module-2-data-handling-in-r",
    "title": "Module 2",
    "section": "",
    "text": "This lesson is based on content from Software Carpentry * https://swcarpentry.github.io/r-novice-inflammation/01-starting-with-data.html * https://datacarpentry.github.io/genomics-r-intro/05-dplyr.html * https://datacarpentry.github.io/r-socialsci/03-dplyr.html * https://datacarpentry.github.io/r-socialsci/04-tidyr.html * https://mqwilber.github.io/2015-04-17-ucsb/lessons/intro_R_lessons/R_data_plotting_analysis.html\n\nChatGPT was used in the creation of examples and explanations.\n\n\n\nBy the end of this session, learners will be able to: * Download data in Terminal to use in RStudio * Differentiate between data frames, lists, and matrices * Use base R to subset data * Use the dplyr package to manipulate dataframes. * Use select() to choose variables from a dataframe. * Use filter() to choose data based on values. * Use group_by() and summarize() to work with subsets of data. * Use mutate() to create new variables.\n\n\n\n\nData frame: most common tabular structure in R\nList: collection of elements of different types\nMatrix: 2D array of only one data type\n\n\n# Data frame\ndf &lt;- data.frame(\n  ID = 1:3,\n  Name = c(\"Alice\", \"Bob\", \"Clara\"),\n  Age = c(34, 45, 29)\n)\n\n# List\npatient &lt;- list(\n  ID = 101,\n  Name = \"David\",\n  Vitals = c(BP = 120, HR = 70),\n  Notes = c(\"No allergies\", \"Nonsmoker\")\n)\n\n# Matrix\nmat &lt;- matrix(1:9, nrow = 3, byrow = TRUE)\n\n# Explore structure\nstr(df)\nstr(patient)\nstr(mat)\n\n\n\n\nFirst, we need to load in the libraries we will use. Remember to go to Terminal to install new packages.\nconda install r-dplyr r-tidyr r-readr\n\nThe tidyverse package is an “umbrella-package” that installs several packages useful for data analysis which work together well such as tidyr, dplyr, readr, ggplot2, tibble, etc. It can take some time to install, so do that later if you would like.\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\n\n#library(tidyverse)\n\nWe are going to download example data from Software Carpentry.\nIn Terminal, navigate to your directory. cd /mnt/work/workbench/&lt;username&gt;/\nExecute the commands to download from a URL wget https://swcarpentry.github.io/r-novice-inflammation/data/r-novice-inflammation-data.zip\nUnzip the data unzip r-novice-inflammation-data.zip\nChange directory into the new folder cd data\nList everything in the directory ls\nGo back to the parent directory: cd ..\nSet your working directory in RStudio in the consoles tab: setwd('/mnt/work/workbench/&lt;username&gt;/scratch/&lt;username&gt;')\nSee your current working directory: getwd()\n\n\n\nNavigate to the following URL. The file should download automatically.\nhttps://swcarpentry.github.io/r-novice-inflammation/data/r-novice-inflammation-data.zip\n\nOpen the .zip file. You should see a folder named \"data\".\n\nRight click on the \"data\" folder, and click \"Copy\".\n\nNavigate to the location you have saved the \"HealthAIinR\" folder.\n\nPaste the \"data\" folder to the \"HealthAIinR\" folder.\n\nNow, go back to RStudio. In the consoles tab on the bottom left, enter the following command to check the current working drive.\n`getwd()`\n\nChange the working directory to the \"data\" folder. (Windows users need the slash / in this direction.)\n`setwd('HealthAIinR/')`\nIf you have used GitHub Desktop to clone the repository to your Windows computer, your path will probably be something like: \n\"C:/Users/&lt;username&gt;/OneDrive - &lt;institution&gt;/Documents/GitHub/HealthAIinR\"\nNow, let’s read our data into R. There are several options for reading csv files:\n\nBase R has the function read.csv() which creates a basic data frame.\nI like to use the data.table package which has a function fread() and can be memory efficient for reading larger datasets. This creates a data.table object which has its own grammar for manipulating.\nThe tidyverse has the readr package with the function read_csv(). This creates a tibble, a special type of data frame.\n\n\nusername&lt;-\"YOUR USERNAME HERE\"\npath&lt;-paste0(\"/mnt/work/workbench/\",username,\"/scratch/\",username,\"/data/\")\npath2&lt;-paste0(path,\"sample.csv\")\n\n#we pass 2 arguments, the path where the file is, and information about if the file has column names\n#we assign the data frame information to the variable \"dat\"\ndatt&lt;-read_csv(file=path2,col_names=TRUE)\n    #for local access from Windows computers (given that the working directory is set to \"HealthAIinR/data\") \n        #NB: Cannot use file paths with the read_csv command on Windows, so working directory needs to be set beforehand\n    #datt&lt;-read_csv(\"sample.csv\", col_names=TRUE)\ndatt\nclass(datt)\n\n#what do you think the col_names option does? how can we find out? what is the default parameter for header? what would happen if we did not use it?\n\n?read_csv\n\n#let's use base R because it's a little easier for beginners\ndat&lt;-read.csv(file=path2,header=TRUE)\n    #for local access from Windows computers (given that the working directory is set to \"HealthAIinR/data\") \n        #NB: Cannot use file paths with the read.csv command on Windows, so working directory needs to be set beforehand\n    #dat&lt;-read.csv(\"sample.csv\", header=TRUE)\nhead(dat)\nclass(dat)\n\nUsing head we see that R has auto-generated column names in the sequence V1 (for “variable 1”), V2, and so on, until V40. We will also inspect our data to get a sense of what it looks like.\n\ndim(dat)\n\nnrow(dat)\n\nnames(dat)\n\nstr(dat)\n\nsummary(dat)\n\nEach row holds the observations for just one patient. Each column holds values for several variables such as gender, blood pressure, and age.\nFor every column in the data frame, the function summary calculates: the minimum value, the first quartile, the median, the mean, the third quartile and the max value, giving helpful details about the sample distribution.\nLike many programming languages, we can use indices to specify a certain row and column. The first value in brackets is row, and the second is column. R starts indices at 1, but some other langauges use 0, so be mindful of this.\nIf we want to select more than one row or column, we can use the function c, which combines the values you give it into one vector or list.\nIf we want to select contiguous rows or columns, we can use the colon.\n\n#this will pull out the value in the 30th row and 20th column\ndat[3,5]\n\ndat[c(1, 3, 5), c(2, 4)]\n\n#let's check what 1:5 does\n1:5\n\n#now let's use it in row and column space\ndat[1:5,1:5]\n\nIf we leave the row or column space open, it means we want to select all of that row or column.\n\n#All columns from row 3 to see the values for Sub003\ndat[3,]\n\n# All rows from column 6-9 regarding Anuerysms\ndat[, 6:9]\n\n#what do you think happens if we leave both values empty\ndat[,]\n\nWe can also use column names. This is helpful because if you add columns or rows later, the indices could change.\n\n#using the $ operator \ndat$BloodPressure\n\n#Name within square brackets\ndat[,\"BloodPressure\"]\n\n#rows can have names too; in this case they're just the row index\ndat[\"1\",]\ndat[1,]\n\nAll the indexing and subsetting that works on data frames also works on vectors.\n\n#pull out a column from the data frame as a vector\nBP_vec&lt;-dat$BloodPressure\n\n#subset for the first 10 participants\nBP_vec[1:10]\n\n\n\n\nCommon dplyr verbs: filter, select, mutate, summarize, group_by &gt; Check out the dplyr cheatsheet\nNote: The packages in the tidyverse, namely dplyr, tidyr and ggplot2 accept both the British (e.g. summarise) and American (e.g. summarize) spelling variants of different function and option names.\n\n\n\n#functionality similar to str()\nglimpse(dat)\n\n#in dplyr we can select the columns by name\nselect(dat,Age)\n\n#another way to do this is with the pipe\ndat %&gt;% select(Age)\n\n# we can use filter to filter observations for individuals with high blood pressure, this time let's save it into a new data frame\nhigh &lt;- dat %&gt;% filter(BloodPressure&gt;130)\nhigh\n\n# what object class is high\nclass(high)\n\nYou may also have noticed that the output from the object high doesn’t run off the screen anymore. It’s one of the advantages of tbl_df (also called tibble), the central data class in the tidyverse, compared to normal dataframes in R.\n\n#you can see we went from 100 to 30 observations when we used filter()\nnrow(dat)\nnrow(high)\n\n# We can also specify multiple conditions within the filter() function.\n\n#high blood pressure and the control group\n\n# To form “and” statements within dplyr, we can pass our desired conditions as arguments in the filter() function, separated by commas:\nhigh_control&lt;-dat %&gt;% filter(BloodPressure&gt;130,Group==\"Control\")\n\n# or we can use the ampersand \"&\" operator\nhigh_control&lt;-dat %&gt;% filter(BloodPressure&gt;130 & Group==\"Control\")\n\n#how can we check that both of these filtered data frames are the same size?\n\n# Have you noticed that the Gender column uses two conventions for male/female (lower case and upper case)?\ntable(dat$Gender)\n\n# To filter for the male participants, we will need to accomodate both conventions\n\n# For an “or” statement, observations must meet at least one of the specified conditions. To form “or” statements we use the logical operator for “or,” which is the vertical bar (|):\n\nmales &lt;- dat %&gt;% filter(Gender==\"M\"|Gender==\"m\")\ntable(males$Gender)",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module 2"
    ]
  },
  {
    "objectID": "modules/Module3_working.html",
    "href": "modules/Module3_working.html",
    "title": "Module 3 Blanks",
    "section": "",
    "text": "Content for this Module is derived from: * https://swcarpentry.github.io/r-novice-gapminder/08-plot-ggplot2.html * https://www.datanovia.com/en/lessons/ggplot-violin-plot/\nChatGPT was used in the creation of examples and explanations.\n\n\n\nBy the end of this session, learners will be able to: * Understand the grammar of graphics * Create basic visualizations using ggplot2 * Customize aesthetics, labels, and themes * Use facet_wrap() to explore subgroups\n\n\n\nThe “gg” in ggplot2 stands for grammar of graphics. The plots are built in layers and you can additively string together functions with + to add layers and customize your plots. The key part of making our plot is to tell ggplot how we want to visually represent the data. We do this by adding a new layer to the plot using one of the geom functions.\n\n#read in the data we used in Module 2\n\nusername&lt;-\"YOUR USERNAME HERE\"\npath&lt;-paste0(\"/mnt/work/workbench/\",username,\"/scratch/\",username,\"/data/\")\npath2&lt;-paste0(path,\"sample.csv\")\n\n\n\n# Start with the dataset and aesthetic mapping\n\n#Nothing appears — because no geometric layer is added yet!\n\n# Add a geometric layer: scatter plot\n\n#the plot will show up on the bottom right of your RStudio IDE",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module 3 Blanks"
    ]
  },
  {
    "objectID": "modules/Module3_working.html#module-3-data-visualization-with-ggplot2",
    "href": "modules/Module3_working.html#module-3-data-visualization-with-ggplot2",
    "title": "Module 3 Blanks",
    "section": "",
    "text": "Content for this Module is derived from: * https://swcarpentry.github.io/r-novice-gapminder/08-plot-ggplot2.html * https://www.datanovia.com/en/lessons/ggplot-violin-plot/\nChatGPT was used in the creation of examples and explanations.\n\n\n\nBy the end of this session, learners will be able to: * Understand the grammar of graphics * Create basic visualizations using ggplot2 * Customize aesthetics, labels, and themes * Use facet_wrap() to explore subgroups\n\n\n\nThe “gg” in ggplot2 stands for grammar of graphics. The plots are built in layers and you can additively string together functions with + to add layers and customize your plots. The key part of making our plot is to tell ggplot how we want to visually represent the data. We do this by adding a new layer to the plot using one of the geom functions.\n\n#read in the data we used in Module 2\n\nusername&lt;-\"YOUR USERNAME HERE\"\npath&lt;-paste0(\"/mnt/work/workbench/\",username,\"/scratch/\",username,\"/data/\")\npath2&lt;-paste0(path,\"sample.csv\")\n\n\n\n# Start with the dataset and aesthetic mapping\n\n#Nothing appears — because no geometric layer is added yet!\n\n# Add a geometric layer: scatter plot\n\n#the plot will show up on the bottom right of your RStudio IDE",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module 3 Blanks"
    ]
  },
  {
    "objectID": "modules/Module5_working.html",
    "href": "modules/Module5_working.html",
    "title": "Module5_working",
    "section": "",
    "text": "Sources\nContent for this Module is derived from * https://rebeccabarter.com/blog/2020-03-25_machine_learning * https://education.rstudio.com/blog/2020/02/conf20-intro-ml/ * https://www.r-bloggers.com/2020/05/using-xgboost-with-tidymodels/#google_vignette * https://rpubs.com/leoohyama/tidymodelstitanic\nChatGPT was used in the creation of examples and explanations.\n\n\nObjectives\nBy the end of the session, participants will be able to: * Understand key concepts in AI & machine learning. * Perform data preprocessing for ML tasks. * Train and evaluate basic models using caret, tidymodels, randomForest, and xgboost. * Interpret key model evaluation metrics.\n\n\nIntroduction\nAI versus Machine Learning\nMachine learning is a subset of AI. “Artificial intelligence refers to the general ability of computers to emulate human thought and perform tasks in real-world environments, while machine learning refers to the technologies and algorithms that enable systems to identify patterns, make decisions, and improve themselves through experience and data.” Deep learning is a type of machine learning using neural networks. You can read more:\n\nhttps://ai.engineering.columbia.edu/ai-vs-machine-learning/\nhttps://professionalprograms.mit.edu/blog/technology/machine-learning-vs-artificial-intelligence/\nhttps://www.ibm.com/think/topics/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks\n\n\n\nMachine learning in R\nThe original package for machine learning in R is caret: Caret stands for Classification And Regression Training. More recently, tidymodels was to developed to use the grammar of the tidyverse as a collection of modelling packages. caret is more widely used while tidymodels offers a modern, modular approach.\nJust as tidyverse has libraries within it such as dplyr and tidyr, tidymodels has libraries within it too. The key ones are:\nrsample:sample splitting (e.g. train/test or cross-validation)\nrecipes:pre-processing\nparsnip: specifying the model\nyardstick: evaluating the model\n\nlibrary(tidymodels)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(randomForest)\nlibrary(ranger)\nlibrary(xgboost)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(workflows)\nlibrary(tune)\nlibrary(mlbench)\n\n\n\nExploratory Data Analysis and Pre-processing\nThe first step is an exploratory data analysis—-plots and summaries of the data to understand the variables and their distributions. However, we have done that in previous modules.\nWe will use the Pima Indian Women’s diabetes dataset which contains information on 768 Pima Indian women’s diabetes status, as well as many predictive features. This is built into the mlbench library.\n\ndata(\"PimaIndiansDiabetes\")\ndf &lt;- PimaIndiansDiabetes\n\n#let's look at the data\n\n\n#histogram of insulin values\n\n#what do you think about the overabundance of values of 0?\n#it turns out that the NA values are coded as 0 in this dataset\n\n#lets convert 0 to NA for the relevant columns\ndf_clean &lt;- df %&gt;%\n  mutate_at(vars(triceps, glucose, pressure, insulin, mass), \n            function(.var) { \n              if_else(condition = (.var == 0), # if true (i.e. the entry is 0)\n                      true = as.numeric(NA),  # replace the value with NA\n                      false = .var # otherwise leave it as it is\n                      )\n            })\n\n\n\ndata split\nTraining data is used for the model training and hyperparameter tuning. Once trained, the model can be evaluated against test data to assess accuracy.\nThis split can be done automatically using the inital_split() function (from rsample) which creates a special “split” object.\n\nset.seed(1234) #set a seed so we have a reproducible split\n\n# split the data into training (75%) and testing (25%)\n\n\n# extract training and testing sets\n\n\n# create cross validation object from training data\n\n\n#how many folds does this make by default?\n\n\n\nDefine a recipe\nCreating a recipe has two parts which we layer with pipes:\n\nSpecify the formula (recipe()): specify the outcome variable and predictor variables\nSpecify pre-processing steps: define the pre-processing steps, such as imputation of missing data, creating dummy variables, scaling, etc.\n\nNote we use the “role selections” as arguments to the pre-processing steps. They specify that we want to apply the step to “all numeric” variables or “all predictor variables”.\n\n# define the recipe\n\n\n\n\n#look at how many predictor variables we’ve specified and the steps we’ve specified \n\n# preprocessing\n\nNote that we used the original df_clean data object rather than the df_train object or the df_split object. We could have used any of these. At this point recipes only takes the names and roles of the outcome and predictor variable from the data object. We will apply this recipe to specific datasets later.\n\n\nSpecify model with parsnip\nparsnip offers a unified interface for the massive variety of models that exist in R. This means that you only have to learn one way of specifying a model, and you can use this specification and have it generate a linear model, a random forest model, a support vector machine model, and more with a single line of code.\nFor model specification you need:\n\nThe model type: what kind of model you want to fit, set using a different function depending on the model, such as rand_forest() for random forest, logistic_reg() for logistic regression, svm_poly() for a polynomial SVM model etc. The full list of models available via parsnip can be found here.\nThe arguments: the model parameter values set using set_args().\nThe engine: the underlying package the model should come from (e.g. “ranger” for the fast C++ implementation of Random Forest or randomForest for the original R implementation), set using set_engine().\nThe mode: the type of prediction - since several packages can do both classification (binary/categorical prediction) and regression (continuous prediction), set using set_mode().\n\nFor random forest, we can tune hyperparameters like mtry, the number of randomly selected predictors to consider at each split, or trees, the number of trees in the forest.\n\n#random forest from ranger\n\n\n\n#Note: If you want to be able to examine the variable importance of your final model later, you will need to set importance argument when setting the engine. For ranger, the importance options are \"impurity\" or \"permutation\".\n\n# logistic regression from glm model\n\n\n\nWorkflow\nNote, it is only when we tune the parameters or fit the model that the recipe and model frameworks are actually implemented.\n\n# Initiate a workflow, add recipe and model to it\n\n\n\nHyperparameter tuning\nWe will perform tuning on the cross-validation object. We will specify the range of mtry values and then we add a tuning layer to the workflow using tune_grid() from the tune package. We will consider two metrics from the yardstick package—accurary and area under the receiver operator curve.\nWe will explore the results with collect_metrics().\n\n# specify which values to try\n\n# extract results\n\n\n# print results\n\nNow let’s also tune the trees hyperparameter at the same time as mtry.\n\n#first we edit the model\n\n#call the workflow again\n\n#now we add trees to the grid\n\n\n# print results\n\n\n# Let's find the maximum AUROC and accuracy\n\n# which parameters do those correspond to?\n\n # Select best by AUROC\n\n\n# Select best by accuracy\n\nAs you can see, the best AUROC and best accuracy give us different hyperparameters. What to do?\nPick the metric that actually matters for your problem * Accuracy is best if false positives and false negatives cost roughly the same and your classes are balanced. * AUROC is better if you care about ranking cases correctly (e.g., risk scores) or you have class imbalance. It evaluates how well the model separates the classes regardless of a specific threshold. * If you’re in biomedical research, AUROC is often more appropriate than accuracy, because outcomes like disease presence are usually imbalanced, and you care about discrimination rather than just the raw percent correct.\nIdeally we would report both metrics but select based on our primary metric of interest. If accuracy and AUROC pull you toward wildly different parameter sets, it may be worth checking why — sometimes it’s due to overfitting to noise in one metric.\n\n# We want to add a layer to our workflow that corresponds to the tuned parameter, i.e. sets mtry to be the value that yielded the best results. If you didn’t tune any parameters, you can skip this step.\n\n# Finalize workflow with best AUROC params\n\n\n\nFit the model\nNow we’ve defined the recipe, model, and tuned the model’s parameters. Since all of this information is in the workflow object, we will apply the last_fit() function to our workflow and our train/test split object. This will automatically train the model specified by the workflow using the training data, and produce evaluations based on the test set.\nNote that the fit object is a data-frame-like object–a tibble with list columns.\nYou can also extract the test set predictions themselves using the collect_predictions() function. Note that there are 192 rows in the predictions object, which corresponds to the number of rows in the test set.\n\n# Extract performance of final model on the test set\n\n# Generate predictions from the test set\n\n# Generate a confusion matrix\n\n\n# Plot distributions of the predicted probability distributions for each class\n\n\n\nPredicting with our final model\nNow that we have our final model, we want to train it on your full dataset and then use it to predict the response for new data.\nThe object from fit contains a few things including the ranger object trained with the parameters established through the workflow based on the joined test/train data (which is in the original clean data frame).\n\n# Let's predict risk of disease for a new patient\n\n\n\nEvaluating Variable Importance\n\n# Extract the underlying ranger model\n\n\n# Get variable importance\n\n\n# Print sorted importance\n\n\n#let's plot the variable importance\n\n#get the results in a data frame for plotting\n\n# point plot\n\n\n\nChallenge:\nThe goal is to use what we have learned with tidymodels, workflow, and tune and run xgboost and glm. Note, XGBoost is robust against highly skewed and/or correlated data, so the amount of preprocessing required is minimal.\n\n# Model specification with tuneable parameters\n\n\n# Preprocessing recipe\n\n# Workflow\n\n# Grid of parameters\n\n\n# Metrics\n\n\n# Tune model\n\n\n# View results\n\n\n# Select best by AUROC\n\n\n# Finalize workflow with best AUROC params\n\n\n# Fit on full training set\n\n\n# Evaluate on test set",
    "crumbs": [
      "Home",
      "Lesson Modules",
      "Module5_working"
    ]
  },
  {
    "objectID": "bonus/Module0.html",
    "href": "bonus/Module0.html",
    "title": "Module 0",
    "section": "",
    "text": "RMarkdown for reproducible research\nBasics of R programming\nUnderstanding the RStudio IDE\nIntroduction to Terminal\n\nContent for this Module is derived from * https://bookdown.org/rdpeng/rprogdatascience/r-nuts-and-bolts.html * https://hbctraining.github.io/Intro-to-R-flipped/lessons/04_introR_packages.html\n\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\n\n\n\nR has five basic or “atomic” classes of objects: - character - numeric (real numbers) - integer - complex - logical (True/False)\nThe arrow is used as the assignment operator.\nR objects can have attributes such as class, dimensions, length, and names.\n\n#this hashtag means we have \"commented out\" this line, so it will not run\n\n#let's define the variable x \nx&lt;-5\n\nx #auto printing occurs\nprint(x) #explicit printing\nclass(x)\n\n#we can coerce an object from one class to another\ny&lt;-as.integer(x)\nclass(y)\n\nz&lt;-1L\nclass(z)\n\nThe most basic type of R object is a vector. Vectors can only contain objects of the same class. A list is represented as a vector but can contain objects of different classes.\n\n#initialize a vector\na&lt;-vector(\"numeric\",10)\nprint(a)\n\n#define a vector with c() function\nb&lt;-c(1,2,3)\nprint(b)\nclass(a)\nclass(b)\n\n#create a data frame\nx&lt;-1:10\ny&lt;-rnorm(10)\ndf&lt;-data.frame(x,y)\n\n#what are the dimensions of the data frame\ndim(df)\n\n#what are the names of the data frame?\nnames(df)\n\nFactors are used to represent categorical data and may be ordered or unordered. These can be useful when you are using regression models.\n\nx&lt;-factor(c(\"male\",\"female\",\"male\",\"female\",\"female\",\"female\"))\n\n#print the factor names of x\nfactor(x)\n\n#how many of each factor are in the vector x\ntable(x)\n\nMissing values are denoted by NA or NaN in R. The function is.na() will return a logical vector indicating which values are NA. If you want to count the number of NA values using table() you will need to use useNA=\"always\" in the function. If you want to calculate a summary statistic such as mean() you have to remove the NA from the vector with na.rm=TRUE.\n\n#let's add an NA value to our factors\ny&lt;-factor(c(x,NA))\nclass(y)\n\nis.na(y)\n\ntable(y,useNA=\"always\")\n\nz&lt;-c(1,20,13,NA,45)\nmean(z) \nmean(z,na.rm=TRUE)\n\n\n\n\nPackages are collections of R functions, data, and compiled code in a well-defined format, created to add specific functionality. There are 10,000+ user contributed packages on CRAN. The directories in R where the packages are stored are called the libraries.\nNormally we can install packages in R using install.packages() function. However, the way the HUNT Cloud workbench is setup, we need to install packages via the Terminal using conda. Go back to the Terminal in the window where your Launcher was. Execute the following commands:\nconda activate r-base conda install ggplot2\n\n#what libraries are loaded in the environment?\nsessionInfo()\n\n#let's install a package; this is commented out because we want to use Terminal in HUNT Cloud Workbench\n#install.packages(\"ggplot2\")\n\n#now we call the package\nlibrary(ggplot2)\n\n#now we can use functions from the ggplot2 package\nggplot(df,aes(x,y)) + geom_point()\n\nCurrently we are operating within RStudio. This is an “Integrated Development Environment” or IDE. It provides a more simplified way to interact with the R programming language. You can see your code, an R console, see the variables in your environment, and see a graphical user interface (GUI) of your file system.\nYou can also see a tab for Terminal. A terminal is a GUI window that takes commands and shows output. This user interface where users type commands and see results on the screen is called a command line interface (CLI).\nIf you want to use R interactively in the Terminal instead of in the RStudio IDE, you can launch a session by typing R.\nWe can also send commands to the Terminal by using the system() command in R. Here, I am listing the contents of the current working directory with the bash command ls. Unfortunately, this also does not work in HUNT Cloud Workbench, but would work when using RStudio on your local computer.\n\n# Unfortunately this won't work in RStudio workbench on HUNT Cloud, so I've commented it out\n#system(\"ls\")\n\nNow let’s open Terminal and practice there. Return to the Launcher and select Terminal.\nHUNT Cloud uses the Linux operating system called Ubuntu. The shell is the software the interprets and executes the commands we type. Common shells are called bash, sh, csh, and tcsh. There are several basic Linux Terminal commands you should know. Copy and paste or type the following commands into the Terminal to see what they do.\nls pwd mkdir cd less test.txt cat test.txt grep \"R\" test.txt echo \"5\" cp test.txt test2.txt mv test2.txt test3.txt rm test3.txt R",
    "crumbs": [
      "Home",
      "Bonus",
      "Module 0"
    ]
  },
  {
    "objectID": "bonus/Module0.html#module-0-introduction-to-r-and-terminal",
    "href": "bonus/Module0.html#module-0-introduction-to-r-and-terminal",
    "title": "Module 0",
    "section": "",
    "text": "RMarkdown for reproducible research\nBasics of R programming\nUnderstanding the RStudio IDE\nIntroduction to Terminal\n\nContent for this Module is derived from * https://bookdown.org/rdpeng/rprogdatascience/r-nuts-and-bolts.html * https://hbctraining.github.io/Intro-to-R-flipped/lessons/04_introR_packages.html\n\n\nThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document.\n\n\n\nR has five basic or “atomic” classes of objects: - character - numeric (real numbers) - integer - complex - logical (True/False)\nThe arrow is used as the assignment operator.\nR objects can have attributes such as class, dimensions, length, and names.\n\n#this hashtag means we have \"commented out\" this line, so it will not run\n\n#let's define the variable x \nx&lt;-5\n\nx #auto printing occurs\nprint(x) #explicit printing\nclass(x)\n\n#we can coerce an object from one class to another\ny&lt;-as.integer(x)\nclass(y)\n\nz&lt;-1L\nclass(z)\n\nThe most basic type of R object is a vector. Vectors can only contain objects of the same class. A list is represented as a vector but can contain objects of different classes.\n\n#initialize a vector\na&lt;-vector(\"numeric\",10)\nprint(a)\n\n#define a vector with c() function\nb&lt;-c(1,2,3)\nprint(b)\nclass(a)\nclass(b)\n\n#create a data frame\nx&lt;-1:10\ny&lt;-rnorm(10)\ndf&lt;-data.frame(x,y)\n\n#what are the dimensions of the data frame\ndim(df)\n\n#what are the names of the data frame?\nnames(df)\n\nFactors are used to represent categorical data and may be ordered or unordered. These can be useful when you are using regression models.\n\nx&lt;-factor(c(\"male\",\"female\",\"male\",\"female\",\"female\",\"female\"))\n\n#print the factor names of x\nfactor(x)\n\n#how many of each factor are in the vector x\ntable(x)\n\nMissing values are denoted by NA or NaN in R. The function is.na() will return a logical vector indicating which values are NA. If you want to count the number of NA values using table() you will need to use useNA=\"always\" in the function. If you want to calculate a summary statistic such as mean() you have to remove the NA from the vector with na.rm=TRUE.\n\n#let's add an NA value to our factors\ny&lt;-factor(c(x,NA))\nclass(y)\n\nis.na(y)\n\ntable(y,useNA=\"always\")\n\nz&lt;-c(1,20,13,NA,45)\nmean(z) \nmean(z,na.rm=TRUE)\n\n\n\n\nPackages are collections of R functions, data, and compiled code in a well-defined format, created to add specific functionality. There are 10,000+ user contributed packages on CRAN. The directories in R where the packages are stored are called the libraries.\nNormally we can install packages in R using install.packages() function. However, the way the HUNT Cloud workbench is setup, we need to install packages via the Terminal using conda. Go back to the Terminal in the window where your Launcher was. Execute the following commands:\nconda activate r-base conda install ggplot2\n\n#what libraries are loaded in the environment?\nsessionInfo()\n\n#let's install a package; this is commented out because we want to use Terminal in HUNT Cloud Workbench\n#install.packages(\"ggplot2\")\n\n#now we call the package\nlibrary(ggplot2)\n\n#now we can use functions from the ggplot2 package\nggplot(df,aes(x,y)) + geom_point()\n\nCurrently we are operating within RStudio. This is an “Integrated Development Environment” or IDE. It provides a more simplified way to interact with the R programming language. You can see your code, an R console, see the variables in your environment, and see a graphical user interface (GUI) of your file system.\nYou can also see a tab for Terminal. A terminal is a GUI window that takes commands and shows output. This user interface where users type commands and see results on the screen is called a command line interface (CLI).\nIf you want to use R interactively in the Terminal instead of in the RStudio IDE, you can launch a session by typing R.\nWe can also send commands to the Terminal by using the system() command in R. Here, I am listing the contents of the current working directory with the bash command ls. Unfortunately, this also does not work in HUNT Cloud Workbench, but would work when using RStudio on your local computer.\n\n# Unfortunately this won't work in RStudio workbench on HUNT Cloud, so I've commented it out\n#system(\"ls\")\n\nNow let’s open Terminal and practice there. Return to the Launcher and select Terminal.\nHUNT Cloud uses the Linux operating system called Ubuntu. The shell is the software the interprets and executes the commands we type. Common shells are called bash, sh, csh, and tcsh. There are several basic Linux Terminal commands you should know. Copy and paste or type the following commands into the Terminal to see what they do.\nls pwd mkdir cd less test.txt cat test.txt grep \"R\" test.txt echo \"5\" cp test.txt test2.txt mv test2.txt test3.txt rm test3.txt R",
    "crumbs": [
      "Home",
      "Bonus",
      "Module 0"
    ]
  },
  {
    "objectID": "bonus/Python_in_R.html",
    "href": "bonus/Python_in_R.html",
    "title": "Python in R",
    "section": "",
    "text": "You can read more about using Pythin in R here: https://www.r-bloggers.com/2020/04/how-to-run-pythons-scikit-learn-in-r-in-5-minutes/\nYou can create a python environment with the packages you want like numpy and scikit-learn.\nIn R Console, you can run python interactively using repl_python(). You will see &gt;&gt;&gt; indicating you are in Python Mode.\n\n#list conda environments\nreticulate::conda_list()\n\n#print versions\npy_config()\n\nNow we are going to use a python code chunk\n\n\n#is python working\n1+1\n\n#import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib as plt\n\n#for scikit-learn module we usually import specific class\nfrom sklearn.ensemble import RandomForestClassifier\n\n#test numpy\nnp.arange(1,10)\n\n#tun a test random forest\nclf = RandomForestClassifier(random_state=0)\nX = [[ 1,  2,  3],  # 2 samples, 3 features\n     [11, 12, 13]]\ny = [0, 1]  # classes of each sample\nclf.fit(X, y)\nclf.predict(X)  # predict classes of the training data",
    "crumbs": [
      "Home",
      "Bonus",
      "Python in R"
    ]
  },
  {
    "objectID": "misc/day2.html#welcome-back",
    "href": "misc/day2.html#welcome-back",
    "title": "day 2",
    "section": "Welcome back!",
    "text": "Welcome back!\nPlease try to log on to Hunt Cloud\n\nConnect to the VPN with tunnelblick\nGo to the course server: https://healthai-course-2025.lab.hdc.ntnu.no/\nOpen a terminal & install more packages:\n\nconda activate r-base\nconda install r-workflows r-tune r-mlbench r-ranger r-randomforest\n\nLaunch an RStudio session\nIf you haven’t, clone the repo:\n\ngit clone https://github.com/bnwolford/HealthAIinR\nIf you have already cloned it, update it: sh   cd /path/to/HealthAIinR   git pull"
  }
]