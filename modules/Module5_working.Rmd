---
title: "Module 5 Working Version"
output: html_document
date: "2025-08-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Sources
Content for this Module is derived from
* https://rebeccabarter.com/blog/2020-03-25_machine_learning
* https://education.rstudio.com/blog/2020/02/conf20-intro-ml/
* https://www.r-bloggers.com/2020/05/using-xgboost-with-tidymodels/#google_vignette
* https://rpubs.com/leoohyama/tidymodelstitanic

ChatGPT was used in the creation of examples and explanations.

### Objectives 
By the end of the session, participants will be able to:
	* Understand key concepts in AI & machine learning.
	* Perform data preprocessing for ML tasks.
	* Train and evaluate basic models using caret, tidymodels, randomForest, and xgboost.
	* Interpret key model evaluation metrics.

### Introduction

AI versus Machine Learning

Machine learning is a subset of AI. "Artificial intelligence refers to the general ability of computers to emulate human thought and perform tasks in real-world environments, while machine learning refers to the technologies and algorithms that enable systems to identify patterns, make decisions, and improve themselves through experience and data." Deep learning is a type of machine learning using neural networks. You can read more:

* https://ai.engineering.columbia.edu/ai-vs-machine-learning/
* https://professionalprograms.mit.edu/blog/technology/machine-learning-vs-artificial-intelligence/
* https://www.ibm.com/think/topics/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks

### Machine learning in R

The original package for machine learning in R is `caret`: Caret stands for Classification And Regression Training. More recently, `tidymodels` was to developed to use the grammar of the tidyverse as a collection of modelling packages. `caret` is more widely used while `tidymodels` offers a modern, modular approach. 

Just as tidyverse has libraries within it such as dplyr and tidyr, tidymodels has libraries within it too. The key ones are:

rsample:sample splitting (e.g. train/test or cross-validation)

recipes:pre-processing

parsnip: specifying the model

yardstick: evaluating the model

```{r}

library(tidymodels)
library(dplyr)
library(readr)
library(tidyr)
library(randomForest)
library(ranger)
library(xgboost)
library(janitor)
library(skimr)
library(workflows)
library(tune)
library(mlbench)


```

### Exploratory Data Analysis and Pre-processing
The first step is an exploratory data analysis—-plots and summaries of the data to understand the variables and their distributions. However, we have done that in previous modules. 

We will use the Pima Indian Women’s diabetes dataset which contains information on 768 Pima Indian women’s diabetes status, as well as many predictive features. This is built into the `mlbench` library.

```{r}
data("PimaIndiansDiabetes")
df <- PimaIndiansDiabetes

#let's look at the data


#histogram of insulin values

#what do you think about the overabundance of values of 0?
#it turns out that the NA values are coded as 0 in this dataset

#lets convert 0 to NA for the relevant columns
df_clean <- df %>%
  mutate_at(vars(triceps, glucose, pressure, insulin, mass), 
            function(.var) { 
              if_else(condition = (.var == 0), # if true (i.e. the entry is 0)
                      true = as.numeric(NA),  # replace the value with NA
                      false = .var # otherwise leave it as it is
                      )
            })

```

### data split 
Training data is used for the model training and hyperparameter tuning. Once trained, the model can be evaluated against test data to assess accuracy.

This split can be done automatically using the inital_split() function (from rsample) which creates a special “split” object.

```{r}

set.seed(1234) #set a seed so we have a reproducible split

# split the data into training (75%) and testing (25%)


# extract training and testing sets


# create cross validation object from training data


#how many folds does this make by default?


```

### Define a recipe

Creating a recipe has two parts which we layer with pipes: 

1. Specify the formula (`recipe()`): specify the outcome variable and predictor variables

2. Specify pre-processing steps: define the pre-processing steps, such as imputation of missing data, creating dummy variables, scaling, etc.

Note we use the “role selections” as arguments to the pre-processing steps. They specify that we want to apply the step to “all numeric” variables or “all predictor variables”.

```{r}

# define the recipe




#look at how many predictor variables we’ve specified and the steps we’ve specified 

# preprocessing


```

Note that we used the original df_clean data object rather than the df_train object or the df_split object. We could have used any of these. At this point recipes only takes the names and roles of the outcome and predictor variable from the data object. We will apply this recipe to specific datasets later.

### Specify model with parsnip

`parsnip` offers a unified interface for the massive variety of models that exist in R. This means that you only have to learn one way of specifying a model, and you can use this specification and have it generate a linear model, a random forest model, a support vector machine model, and more with a single line of code.

For model specification you need:

1. The model type: what kind of model you want to fit, set using a different function depending on the model, such as `rand_forest()` for random forest, `logistic_reg()` for logistic regression, `svm_poly()` for a polynomial SVM model etc. The full list of models available via parsnip can be found [here](https://tidymodels.github.io/parsnip/articles/articles/Models.html).

2. The arguments: the model parameter values set using `set_args()`.

3. The engine: the underlying package the model should come from (e.g. “ranger” for the fast C++ implementation of Random Forest or randomForest for the original R implementation), set using `set_engine()`.

4. The mode: the type of prediction - since several packages can do both classification (binary/categorical prediction) and regression (continuous prediction), set using `set_mode()`.

For random forest, we can tune hyperparameters like mtry, the number of randomly selected predictors to consider at each split, or trees, the number of trees in the forest.

```{r}

#random forest from ranger



#Note: If you want to be able to examine the variable importance of your final model later, you will need to set importance argument when setting the engine. For ranger, the importance options are "impurity" or "permutation".

# logistic regression from glm model




```


### Workflow

Note, it is only when we tune the parameters or fit the model that the recipe and model frameworks are actually implemented.

```{r}

# Initiate a workflow, add recipe and model to it


```

### Hyperparameter tuning

We will perform tuning on the cross-validation object. We will specify the range of mtry values and then we add a tuning layer to the workflow using `tune_grid()` from the `tune` package. We will consider two metrics from the `yardstick` package—accurary and area under the receiver operator curve.

We will explore the results with `collect_metrics()`.

```{r}

# specify which values to try

# extract results


# print results
```

Now let's also tune the trees hyperparameter at the same time as mtry.

```{r}
#first we edit the model

#call the workflow again

#now we add trees to the grid


# print results


# Let's find the maximum AUROC and accuracy

# which parameters do those correspond to?

 # Select best by AUROC


# Select best by accuracy


```

As you can see, the best AUROC and best accuracy give us different hyperparameters. What to do?

Pick the metric that actually matters for your problem
	*	Accuracy is best if false positives and false negatives cost roughly the same and your classes are balanced.
	*	AUROC is better if you care about ranking cases correctly (e.g., risk scores) or you have class imbalance. It evaluates how well the model separates the classes regardless of a specific threshold.
	*	If you’re in biomedical research, AUROC is often more appropriate than accuracy, because outcomes like disease presence are usually imbalanced, and you care about discrimination rather than just the raw percent correct.

Ideally we would report both metrics but select based on our primary metric of interest. If accuracy and AUROC pull you toward wildly different parameter sets, it may be worth checking why — sometimes it’s due to overfitting to noise in one metric.

```{r}

# We want to add a layer to our workflow that corresponds to the tuned parameter, i.e. sets mtry to be the value that yielded the best results. If you didn’t tune any parameters, you can skip this step.

# Finalize workflow with best AUROC params

```


### Fit the model

Now we’ve defined the recipe, model, and tuned the model’s parameters. Since all of this information is in the workflow object, we will apply the `last_fit()` function to our workflow and our train/test split object. This will automatically train the model specified by the workflow using the training data, and produce evaluations based on the test set.

Note that the fit object is a data-frame-like object--a tibble with list columns.

You can also extract the test set predictions themselves using the `collect_predictions()` function. Note that there are 192 rows in the predictions object, which corresponds to the number of rows in the test set.

```{r}




# Extract performance of final model on the test set

# Generate predictions from the test set

# Generate a confusion matrix


# Plot distributions of the predicted probability distributions for each class


```

### Predicting with our final model

Now that we have our final model, we want to train it on your full dataset and then use it to predict the response for new data.

The object from fit contains a few things including the ranger object trained with the parameters established through the workflow based on the joined test/train data (which is in the original clean data frame).

```{r}

# Let's predict risk of disease for a new patient



```

### Evaluating Variable Importance

```{r}

# Extract the underlying ranger model


# Get variable importance


# Print sorted importance


#let's plot the variable importance

#get the results in a data frame for plotting

# point plot

```


### Challenge:

The goal is to use what we have learned with `tidymodels`, `workflow`, and `tune` and run xgboost and glm. Note, XGBoost is robust against highly skewed and/or correlated data, so the amount of preprocessing required is minimal. 

```{r}


# Model specification with tuneable parameters


# Preprocessing recipe

# Workflow

# Grid of parameters


# Metrics


# Tune model


# View results


# Select best by AUROC


# Finalize workflow with best AUROC params


# Fit on full training set


# Evaluate on test set




```


